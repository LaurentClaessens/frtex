% This is part of Agregation : modélisation
% Copyright (c) 2011
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\begin{corrige}{Model-0004}

    \begin{enumerate}
        \item
            Nous calculons
            \begin{equation}
                \begin{aligned}[]
                    E(\hat\sigma_n^2)&=\frac{1}{ n }\sum_i E\big[ (X_i-m)^2 \big]\\
                    &=\frac{1}{ n }\sum_i\Var(X_i-m)-\frac{1}{ n }\sum_iE(X_i-mj^2)\\
                    &=\frac{1}{ n }\sum_i\Var(X_i)\\
                    &=\sigma^2.
                \end{aligned}
            \end{equation}
        \item
            Par la formule \eqref{EqRisqueetbaisiVar} nous savons que le risque d'un estimateur sans biais est donné par sa variance:
            \begin{equation}
                R(\hat\theta_n,\theta)=\Var(| \hat\sigma_n^2-\theta |)=\Var(\hat\sigma_n^2).
            \end{equation}
            Étant donné que les variables aléatoires \( X_i\) sont indépendantes et identiquement distribuées, le lemme \ref{LemVarXpYsmindep} nous enseigne que la variance de la somme est la somme des variances. Nous avons donc à calculer
            \begin{equation}
                \begin{aligned}[]
                    \Var(| \hat\sigma_n^2-\theta |)&=\Var(\hat\sigma_n^2)\\
                    &=\frac{1}{ n^2 }\sum_i\Var\big[ (X_i-m)^2 \big]\\
                    &=\frac{1}{ n^2 }\sum_i E\big[ (X_i-m)^4 \big]-E\big[ (X_i-m)^2 \big]^2.
                \end{aligned}
            \end{equation}
            Ces espérances ne sont pas très compliquées à calculer en utilisant la fonction caractéristique donnée par la proposition \ref{PropFnCaractNorm} :
            \begin{equation}
                \Phi_{X-m}(t)= E\big(  e^{it(X-m)} \big)= e^{-itm}E( e^{itX})= e^{-itm}\Phi_X(t)= \exp\left( -\frac{ \sigma^2t^2 }{2} \right).
            \end{equation}
            Nous avons \( \Phi^{(4)}(0)=3\sigma^4\) et \( \Phi''(0)=-\sigma^2\). Par conséquent
            \begin{equation}
                \Var(\hat\sigma_n^2)=\frac{1}{ n^2 }\sum_i2\theta^2=\frac{ 2\theta^2 }{ n }.
            \end{equation}
            Notez ici que \( \theta=\sigma^2\).

        \item
            En tenant compte du fait que \( \Var(\hat\sigma_n^2)=\frac{ 2\theta^2 }{ n }\) et \( E(\hat\sigma_n^2)=\theta\), nous avons
            \begin{subequations}
                \begin{align}
                    E\big( (c\hat\sigma_n^2-\theta)^2 \big)&=\Var(c\hat\sigma_n^2-\theta)+E(c\hat\sigma_n^2-\theta)^2\\
                    &=2\frac{ c^2\theta^2 }{ n }+(c-1)^2\theta^2.   \label{subeqcdtedestcthecmu}
                \end{align}
            \end{subequations}
            La dérivée (par rapport à \( c\)) de cela s'annule pour \( c_0=\frac{ n }{ n+2 }\). Notons que nous n'avons pas tout à fait démontré que cela est bien un minimum. Calculons cependant le risque quadratique de notre estimateur pour cette valeur de \( c\). Pour cela nous reportons \( c=c_0\) dans l'expression \eqref{subeqcdtedestcthecmu}:
            \begin{equation}
                E\big( \frac{ n }{ n+2 }\hat\sigma_n^2 \big)=\frac{ 2\theta^2 }{ n+2 }.
            \end{equation}
            Cela est effectivement plus petit que \( R(\hat\sigma_n^2,\theta)\).
    \end{enumerate}
    Nous avons ainsi construit un estimateur biaisé qui a un risque quadratique plus petit que l'estimateur non biaisé.

\end{corrige}
