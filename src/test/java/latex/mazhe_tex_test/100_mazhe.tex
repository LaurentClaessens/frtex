% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\subsection{Cartan criterion}
%----------------------------

Let us recall a result: $\dD\lG=\lG^1$, $[\dD\lG,\dD\lG]\subset\lG^2$; then $\dD^k\lG\subset\lG^k$. Thus if $\lG$ is nilpotent, it is solvable. On the other hand, by the Engel theorem \ref{tho:Engel}, $\dD\lG$ is nilpotent if and only if all the $\ad_{\dD\lG}x$ are nilpotent for $x\in\dD\lG$.


\begin{theorem}[Cartan criterion]
Let $\lG$ be a subalgebra of $\gl(V)$. We suppose that $\tr(xy)=0$ $\forall x\in\dD\lG, y\in\lG$. Then $\lG$ is solvable.
\end{theorem}

\begin{proof}
It is sufficient to prove that $\dD\lG$ is nilpotent indeed if we write $\dD^k\lG\subset\lG^k$ with $\dD\lG$ instead of $\lG$, $\dD^{k+1}\lG\subset(\dD\lG)^k$. If $\dD\lG$ is nilpotent, $(\dD\lG)^n=0$ and $\dD^{n+1}\lG=0$ so that $\lG$ is solvable. 

Let us consider $x\in\dD\lG$. We have to prove that it is ad-nilpotent (see the Engel theorem \ref{tho:Engel}). Let $A=\dD\lG$, $B=\lG$ and $M=\{x\in\gl(V)\tq [x\lG]\subset\dD\lG\}$. By definition of $\dD\lG$, $\lG\subset M$. The lemma \ref{lem:M_nil} will conclude that $x\in\dD\lG$ is nilpotent if $\tr(xy)=0$ for any $y\in M$. Here we just have this equality for $y\in\lG$. 

A typical generator of $\dD\lG$ is $[x,y]$ with $x$, $y\in\lG$. Take a $z\in M$; by the formula $\tr([x,y]z)=\tr(x[y,z])$, the trace that we have to check is
\begin{equation}
  \tr([x,y]z)=\tr(x[y,z])
             =\tr([y,z]x).
\end{equation}
But with $z\in M$, $[y,z]\in\dD\lG$, then $\tr([x,y]z)=\tr([y,z]x)=0$. Thus we are in the situation of the lemma.
\end{proof}


\begin{corollary}\label{cor:ad_g_sol}
A Lie algebra $\lG$ for which $\tr(\ad x\circ\ad y)=0$ for all $x\in\dD\lG$, $y\in\lG$ is solvable.
\end{corollary}

\begin{proof}
We consider $\lH=\ad\lG$; this is a subalgebra of $\gl(V)$ such that $a\in\dD\lH$ and $b\in\lH$ imply $\tr(ab)=0$. In order to see it, remark that $a\in\dD\lH$ can be written as $a=[\ad x,\ad y]=\ad[x,y]$ for certain $x$, $y\in\lG$. Then $\tr(ab)=\tr(\ad[x,y]\ad z)$ with $x$, $y$, $z\in\lG$; this is zero from the hypothesis. Then $\lH=\ad\lG$ is solvable.

It is also known that $\ker(\ad)=\mZ(\lG)$ is also solvable. Now we consider $\lM$ a complementary of $\mZ(\lG)$ in $\lG$ : $\lG=\mZ\oplus\lM$. The Lie algebra $\ad(\lM)$ is solvable and the homomorphism $\dpt{\phi}{\ad\lM}{\lM}$ defined by $\phi(\ad x)=x$ is well defined. From the first item of the proposition \ref{prop:trois_resoluble}, $\lM$ is solvable. With obvious notations, an element of $\dD\lM$ can be written as $[m,m']$ (because $\mZ(\lG)$ don't contribute to $\dD\lG$). Then $\dD\lG=\dD\lM$, so that $\lG$ is as much solvable than $\lM$.
\end{proof}


\begin{lemma}
The radical of a Lie algebra is non zero if and only if it has at least non zero abelian ideal.
\label{lem:ss_ideal}
\end{lemma}

\begin{proof}
The radical of $\lG$ is its unique maximal solvable ideal. An eventually non empty abelian ideal should be in the radical.

Let us now consider that the radical is non zero, and consider the derived series of $\Rad\lG$. Since $\Rad\lG$ is solvable, we can consider $n$, the minimal integer such that $\dD^n\Rad\lG=0$. Then $\dD^{n-1}\Rad\lG$ is a non zero abelian ideal.
\end{proof}


\begin{theorem}     \label{ThoRadicalEquivSS}
A Lie algebra is semisimple if and only if its radical is zero.
\end{theorem}

\begin{proof}
\subdem{Direct sense}
We suppose $\Rad\lG=0$ and we consider $S$, the radical of the Killing form :
\[
   S=\{X\in\lG\tq B(X,Y)=0\,\forall Y\in\lG\}.
\]
By definition, for any $X\in S$ and $Y\in\lG$, $\tr(\ad X\circ\ad Y)=0$. The Cartan criterion makes $\ad S$ solvable and the corollary \ref{cor:ad_g_sol} makes $S$ solvable.

Now, the $ad$-invariance of the Killing form turns $S$ into an ideal, so that $S\subset\Rad(\lG)$ because any solvable ideal is contained in $\Rad\lG$. From the assumptions, $\Rad S=0$, then $S\subset\Rad\lG=0$. This shows that the Killing form is nondegenerate.

\subdem{Inverse sense}
We suppose $S=0$ and we will show that any abelian ideal of $\lG$ is in $S$. In this case, if $A$ is a solvable ideal with $\dD^nA=0$, then $\dD^{n-1}A$ is an abelian ideal, so that $\dD^{n-1}A=0$. By induction, $A=0$.

Let $I$ be an abelian ideal of $\lG$, $X\in I$ and $Y\in\lG$. Then $\ad X\circ\ad Y$ is nilpotent because for $Z\in\lG$,
\begin{equation}
  (\ad X\ad Y\ad X\ad Y)Z=(\ad X\ad Y)\underbrace{( [X,[Y,Z]] )}_{=X_1\in I}
                         =(\ad X)\underbrace{[Y,X_1]}_{=X_2\in I}
             =(\ad X)X_2
             =0.
\end{equation}
Then $0=\tr(\ad X\ad Y)=B(X,Y)$ and $X\in S$, so that $I\subset S=0$.

\end{proof}

\subsection{More about radical}
%-------------------------------

If $\lG$ is a Lie algebra whose radical is $\lR$, we say that a subalgebra $\lS$ of $\lG$ is a \defe{Levi subalgebra}{levi subalgebra} if $\lG=\lR\oplus\lS$.

Any Lie algebra posses a Levi subalgebra\quext{Reference needed.}.

\begin{lemma}
If $\lA$ is an ideal in a Lie algebra $\lG$, then 
\[
  \Rad\lA=(\Rad\lR)\cap\lA.
\]
\label{lem:rad_ideal}
\end{lemma}

Before to begin the proof, let us recall that lemma \ref{lem:pre_trois_resoluble} gives us an isomorphism $\dpt{\psi}{(\lA+\lB)/\lA}{\lB/(\lA\cap\lB)}$ when $\lA$ and $\lB$ are ideals in $\lG$.

\begin{proof}[Proof of the lemma]
If $\lR$ is the radical of $\lG$, then the radical of $\lG/\lR$ is zero, so that $\lR/\lR$ is semisimple. Let $\lA$ be an ideal in $\lG$, then $(\lA+\lR)/\lR$ is an ideal in the semisimple Lie algebra $\lG\lR$, so that it is also semisimple. From the isomorphism, $\lA/(\lA\cap\lR)$ is also semisimple and $\lA\cap\lR$ must contains the radical of $\lA$. Indeed if a solvable ideal of $\lA$ where not in $\lA\cap\lR$, then this should give rise to a non zero solvable ideal in $\lA/(\lA\cap\lR)$ although the latter is semisimple. Then $\lA\cap\lR=\Rad\lA$.
\end{proof}

\begin{proposition}
If $A$ is a compact group of automorphisms of the Lie algebra $\lG$, then there exists a Levi subalgebra of $\lG$ which is invariant under $A$.
\end{proposition}

\begin{proof}
Let $\lR$ be the radical of $\lG$; we will split our proof into two cases following $[\lR,\lR]=0$ or not.
\subdem{The radical is abelian}
In this first case we consider an induction with respect to the dimension of $\lG$. We consider $\olG=\lG/\lRlR$ and $\olR=\lR/\lRlR$ : these are algebras with one less dimension that $\lG$ and $\lR$. We denote by $\dpt{\pi}{\lG}{\olG}$ the natural projection.

We begin to prove that $\olR$ is the radical of $\olG$. It is clear from the Lie algebra structure on a quotient that $\olR$ is an ideal because $\lR$ is. It is also clear that $\olR$ is solvable. We just have to see that $\olR$ is maximal in $\olG$. For this, suppose that $\olR\cup\oX$ is a solvable ideal in $\olG$. Then it is easy to see that $\lR\cup X$ is an ideal in $\lG$. Taking commutators in $\olR\cup\oX$, we always finish in $\overline{0}\in\olG$, i.e. in $\lRlR$. Taking again some commutators, we finish on $0\in\lG$ because $\lR$ is solvable. This contradict the maximality of $\lR$.

Since $A$ is made up of automorphisms, it leaves $\lR$ invariant, so that it also acts on $\olG$ as an automorphism group: $a\oX=\overline{aX}$ for $a\in A$ and $X\in\lG$. From the induction assumption, we can find a Levi subalgebra $\olS$ in $\olG$ : $\olS\oplus\olR=\olG$. In this case, the radical of $\pi^{-1}(\olS)$ is $\lRlR$. Indeed in the one hand, $\olR\cap\olS=0$, so that $\pi^{-1}(\olR\cap\olS)=\lRlR$. In the other hand $\pi^{-1}(\olR\cap\olS)=\pi^{-1}(\olR)\cap\pi^{-1}(\olS)=\lR\cap\pi^{-1}(\olS)$. The lemma \ref{lem:rad_ideal} conclude that $\Rad\pi^{-1}(\olS)=\lRlR$.

Now $A$ is a compact group of automorphism which leaves invariant $\pi^{-1}(\olS)$, so we have a Levi subalgebra $\lS$ of $\pi^{-1}(\olS)$ invariant under $A$. We will see that this is in fact a Levi subalgebra of the whole $\lG$, i.e. we have to prove that $\lS\oplus\lR=\lG$. From the definition of $\lS$, 
\[
   \lS\oplus\lRlR=\pi^{-1}(\olS),
\]
and by definition of $\olS$,
\[
   \olS\oplus\frac{\lR}{\lRlR}=\olG.
\]
Then
\begin{equation}
  \lG=\pi^{-1}(\olS)\oplus\lR+\lRlR
     =\lS\oplus\lRlR\oplus\lR+\lRlR
     =\lS\oplus\lR.
\end{equation}

We can now pass to the second case: $\lRlR=0$.
\subdem{The radical is not abelian}
Let $\lS_0$ and $\lS$ be Levi subalgebras of $\lG$. For $X\in\lS_0$, we write
\[
   X=f(X)+X_{\lS}
\]
with respect to the decomposition $\lG=\lS\oplus\lR$. This defines a linear map $\dpt{f}{\lS_0}{\lR}$. For any $X$, $Y\in\lS_0$, $[X_{\lS},X_{\lS}]=[X,Y]-[X,f(y)]-[f(X),Y]$ because $\lR$ is abelian. Since\quext{C'est pas clair pourquoi on a \c a.}, $[X_{\lS},X_{\lS}]=[X,Y]_{\lS}$,
\begin{equation}\label{eq:f_presque_isom}
f([X,Y])=[X,f(Y)]-[f(X),Y].
\end{equation}
Now let us consider a map $\dpt{f}{\lS_0}{\lR}$ which satisfy this equation. Then the map $X\to X-f(X)$ is an isomorphism between $\lS_0$ and his image which is a Levi subalgebra of $\lG$. Indeed
\begin{equation}
\begin{split}
[X,Y]&\to[X,Y]-f([X,Y])\\
     &=[X,Y]-[X,f(Y)]-[f(X),Y]\\
     &=[X-f(X),Y-f(Y)].
\end{split}
\end{equation}
Now we consider $V$, the space of all the linear maps $\lS_0\to\lR$ which fulfil the condition \eqref{eq:f_presque_isom}. We have a bijection between $V$ and the Levi subalgebras of $\lG$ : for any Levi subalgebra we associate the map $f\in V$ given by $X=f(X)+X_{\lS}$.

So our proof can be reduced to find a fixed point of $V$ under the action of $A$. In order to do that, we will see that $A$ is a group of \emph{affine} transformations on $V$. Consider a $\alpha\in A$ and $f_0$, $f_0^{\alpha}$, $f^{\alpha}$ be the elements of $V$ corresponding to $\lS_0$, $\lS$ and $\alpha(\lS)$. We take a $X\in\lS_0$ and we denote by $\oalpha(X)$ the $\lS_0$-component of $\alpha(X)$ with respect to the decomposition $\lG=\lR\oplus\lS_0$ :
\[
   \alpha(X)=\oalpha(X)+\beta(X).
\]
This also defines $\dpt{\beta}{\lG}{\lR}$ and $-\beta(X)$ is the $\lR$-component of $\oalpha(X)$ with respect to $\lG=\lR\oplus\alpha(\lS_0)$. Since $f_0^{\alpha}$ just correspond to this decomposition, $f_0^{\alpha}(\oalpha(X))=-\beta(X)$, so that 
\begin{equation}
\begin{split}
\oalpha(X)&=f_0^{\alpha}(\oalpha(X))+\alpha(X)\\
          &=f_0^{\alpha}(\oalpha(X))+\alpha(f(X))-\alpha(f(X))+\alpha(X).
\end{split}
\end{equation}
Since $X-f(X)\in\lS$, $\alpha(X)-\alpha(f(X))\in\alpha(\lS)$, then $f_0^{\alpha}(\oalpha(X))+\alpha(X)$ is the $\lR$-component of $\oalpha(X)$ with respect to $\lG=\lR\oplus\alpha(\lS)$. Then 
\[
   f_0^{\alpha}(\oalpha(X))+\alpha(f(X))=f^{\alpha}(\oalpha(X))=f^{\alpha}(\oalpha(X)).
\]
Since $X$ was taken arbitrary, $f^{\alpha}=f^{\alpha}_0+\alpha\circ f\circ\oalpha^{-1}$. Then the map $V\to V$, $f\to f^{\alpha}$ is an affine transformation with translation equals to $f_0^{\alpha}$ and linear part being $f\to\alpha\circ f\circ\oalpha$.

A general result shows that a compact group of affine transformations on a vector space has a fixed point.

\end{proof}

An other result that will be used :
\begin{lemma}		\label{lem:Killing_ss_descent}
If $G$ is a semisimple Lie group and $H$ a semisimple subgroup of $G$, the restrictions on $H$ of the Killing form of $G$ is nondegenerate.
\end{lemma}
%TODO : a proof.

\subsection{Compact Lie algebra}\label{pg:compact_Lie}
%-------------------------------

We consider $\lG$, a real Lie algebra and $\lH$, a subalgebra of $\lG$. Let $K^*$ be the analytic subgroup of $\Int(\lG)$ which corresponds to the subalgebra $\ad_{\lG}(\lH)$ of $\ad_{\lG}(\lG)$. 

\begin{definition}
We say that $\lH$ is \defe{compactly embedded}{compactly embedded} in $\lG$ if $K^*$ is compact. A Lie algebra is \defe{compact}{compact!Lie algebra}\index{Lie!algebra!compact} when it is compactly embedded in itself.
\end{definition}

The analytic subgroup of $\Int(\lG)$ which corresponds to $\ad_{\lG}(\lG)$, by definition, is $\Int(\lG)$. Then the compactness of $\lG$ is the one of $\Int(\lG)$.

\begin{remark}
The compactness notion on a Lie group is defined from the topological structure of the Lie group seen as a manifold. It is all but trivial that the compactness on a Lie group is related to the compactness on its Lie algebra; the proposition \ref{prop:alg_grp_compact} will however make the two notions related in the natural way.
\end{remark}

\begin{remark}
The topology on $K^*$ is not necessary the same as the induced one from $\Int(\lG)$ and $\Int(\lG)$ has also not necessary the induced topology from $\GL(\lG)$. However the next proposition will show that the compactness notion is well the one induced from $\GL(\lG)$.
\end{remark}

\begin{proposition}
We consider $\tK$, the same set and group as $K^*$, but with the induced topology from $\GL(\lG)$. Then $\tK$ is compact if and only if $K^*$ is compact.
\end{proposition}

Note however that $K^*$ and $\tK$ are not automatically the same as manifold.

\begin{proof}
\subdem{$K^*$ compact implies $\tK$ compact}
The identity map $\dpt{\iota}{K^*}{\GL(\lG)}$ is analytic, and then is continuous because $\Int(\lG)$ is by definition an analytic subgroup of $\GL(\lG)$ and $K^*$ an analytic subgroup of $\Int(\lG)$. If we have a covering of $\tK$ with open set $\mO_i\cap\tK$ of $\tK$ ($\mO_i$ is open in $\GL(\lG)$), the continuity of $\iota$ make the finite subcovering of $K^*$ good for $\tK$.
\subdem{$\tK$ compact implies $K^*$ compact}
If $\tK$ is compact, then it is closed in $\GL(\lG)$. As set, $K^*$ is closed in $\GL(\lG)$ and by definition it is connected. Then by the theorem \ref{tho:H_ferme}, $K^*$ is a topological subgroup of $\GL(\lG)$. Consequently, $K^*$ and $\tK$ are homeomorphic and they have same topology.
\end{proof}

A lemma without proof\quext{J'ai m\^eme pas trouv\'e d'\'enonc\'e de ce th\'eor\`eme.}.
\begin{lemma}
If $G$ is a compact group in $\GL(n,\eR)$, then there exists a $G$-invariant quadratic form on $\eR^n$.
\end{lemma}

\begin{proposition}     \label{ProplGcompactKillNeg}
Let $\lG$ be a real Lie algebra.

\begin{enumerate}
\item If $\lG$ is semisimple, then $\lG$ is compact if and only if  the Killing form is strictly negative definite.
\item If it is compact then it is a direct sum
\begin{equation}
   \lG=\mZ\oplus [\lG,\lG]
\end{equation}
where $\mZ$ is the center of $\lG$ and the ideal $[\lG,\lG]$ is compact and semisimple.
\end{enumerate}
\label{prop:compact_Killing}
\end{proposition}

\begin{proof}
\subdem{If the Killing form is nondegenerate}
We consider $\lG$, a Lie algebra whose Killing form is strictly negative definite. Up to some dilatations (and a sign), this is the euclidian metric. Then $O(B)$, the group of linear transformations which leave $B$ unchanged is compact in the topology of $\GL(\lG)$ : this is almost the rotations. From equation \eqref{eq:Aut_Iso}, $\Aut(\lG)\subset O(B)$. With this, $\Aut(\lG)$ is closed in a compact, then it is compact. Then $\Int(\lG)$ is closed in $\Aut(\lG)$ --here is the assumption of semi-simplicity-- and $\Int(\lG)$ is compact.
\subdem{If $\lG$ is compact}
Since $\lG$ is compact, $\Int(\lG)$ is compact in the topology of $\Aut(\lG)$; then there exists an $\Int(\lG)$-invariant quadratic form $Q$. In a suitable basis $\{X_1,\ldots,X_n\}$ of $\lG$, we can write this form as
\[
   Q(X)=\sum x_i^2
\]
for $X=\sum x_iX_i$. In this basis the elements of $\Int(\lG)$ are orthogonal matrices and the matrices of $\ad(\lG)$ are skew-symmetric matrices (the Lie algebra of orthogonal matrices). Let us consider a $X\in\lG$ and denote by $a_{ij}(X)$ the matrix of $\ad(X)$. We have
\begin{equation}
\begin{split}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_i\sum_ja_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2\leq 0.
\end{split}
\end{equation}
Then the Killing form is negative definite\footnote{Here we use ``negative definite''\ and ``\emph{strictly} negative definite''; in some literature, the terminology is slightly different and one says ``\emph{semi} negative definite''\ and ``negative definite''.}. On the other hand, $B(X,X)=0$ implies $\ad(X)=0$ and $X\in\mZ(\lG)$. Thus $\lG^{\perp}\subset\mZ$. If $\lG$ is semisimple, this center is zero; this conclude the first item of the proposition.

Now $\mZ$ is an ideal and corollary \ref{cor:decomp_ideal} decomposes $\lG$ as
\begin{equation}
  \lG=\mZ\oplus\lG'.
\end{equation}
Let us suppose that the restriction of $B$ to $\lG'\times\lG'$ is actually the Killing form on $\lG'$ (we will prove it below). Then the Killing form on $\lG'$ is strictly negative definite; then $\lG'$ is compact.

Now we prove that the Killing form on $\lG$ descent to the Killing form on~$\lG'$. Remark that $\mZ$ is invariant under all the automorphism. Indeed consider $Z\in\mZ$, i.e.  $[X,Z]=0$. If $\sigma$ is an automorphism, 
\[
   [X,\sigma Z]=\sigma[\sigma^{-1} X,Z]=0.
\]
Here the difference between $\Int(\lG)$ and $\Aut(\lG)$ is the fact that $\Int(\lG)$ is compact; then we can construct a $\Int(\lG)$-invariant quadratic form $Q$, but not a $\Aut(\lG)$-invariant one. We consider an orthogonal complement (with respect to $Q$) $\lG'$ of $\mZ$:
\begin{equation}
   \lG=\lG'\oplus_{\perp}\mZ.
\end{equation}
The algebra $\lG'$ is also invariant because for any $Z\in\mZ$, 
\[
Q(Z,\sigma X)=Q(\sigma^{-1}(Z),X)=0.
\]
It is also clear that $\mZ$ is invariant under $\ad\lG$ because $(\ad X)Z=0$. Finally $\lG'$ is invariant as well under $\ad(\lG)$. Indeed $a\in\ad(\lG)$ can be written as $a=a'(0)$ for a path $a(t)\in\Int(\lG)$. We identify $\lG$ and his tangent space (as vector spaces),
\[
  aX=\Dsdd{ a(t)X }{t}{0}.
\]
If $X\in\lG'$, $a(t)X\in\lG'$ for any $t$ because $\lG'$ is invariant under $\Int(\lG)$\footnote{As physical interpretation, if something is invariant under a group of transformations, it is invariant under the infinitesimal transformations as well.}. Thus $a(t)X$ is a path in $\lG'$ and his derivative is a vector in $\lG'$. 

All this make $\lG'$ an ideal in $\lG$; then the Killing form descent by lemma \ref{lem:Killing_descent_ideal}. Now if $X\in\lG$, we have
\begin{equation}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_{ij}a_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2;
\end{equation}
then $B(X,X)\leq 0$ and the equality holds if and only if $\ad X=0$ i.e. if and only if $X\in\mZ$. Thus $B$ is strictly negative definite on $\lG'$.

Up to now we have proved that $\lG'$ is semisimple (because $B$ is nondegenerate) and compact (because $B$ is strictly negative definite).

It remains to be proved that $\lG'=[\lG,\lG]=\dD(\lG)$. From corollary \ref{cor:decomp_ideal}, $\dD\lG$ has a complementary $\lA$ which is also an ideal: $\lG=\dD\lG+\lA$. Then $[\lG,\lA]\subset\dD\lG$ and $[\lG,\lA]\subset\lA\cap\dD\lG:\{0\}$. Then $\lA\subset\mZ$, so that
\begin{align}\label{eq:G_Z_B}
   \lG=\mZ+\dD\lG&&\text{(non direct sum)}.
\end{align}
Now we have to prove that the sum is actually direct. The ideal $\mZ$ has a complementary ideal $\lB$ : $\lG=\mZ\oplus\lB$ and
\[
   \dD\lG=[\lG,\lG]\subset\underbrace{[\lG,\mZ]}_{=0}+[\lG,\lB]\subset\lB.
\]
Then $\dD\lG\subset\lB$ which implies that $\dD\lG\cap\mZ=\{0\}$ because the sum $\lG=\mZ\oplus\lB$ is direct. Then the sum \eqref{eq:G_Z_B} is direct.

\end{proof}

\begin{proposition}
A real Lie algebra $\lG$ is compact if and only if one can find a compact Lie group $G$ which Lie algebra is isomorphic to $\lG$.
\label{prop:alg_grp_compact}
\end{proposition}

\begin{proof}
\subdem{Direct sense} Since $\lG$ is compact, $\lG=\mZ\oplus\dD\lG$ with $\dD\lG=\lG'$ compact and semisimple; in particular, the center of $\lG'$ is $\{0\}$. Since $\mZ$ is compact and abelian, it is isomorphic to the torus $S^1\times\ldots\times S^1$. Since $\lG'$ is compact, $\Int(\lG')$ is compact, but the Lie algebra if $\Int(\lG')$ is --by definition--  $\ad(\lG')$. The center of a semisimple Lie algebra is zero; then $\ad X'=0$ implies $X=0$ (for $X\in\lG'$). Then $\ad$ is an isomorphism between $\lG'$ and $\ad\lG'$.

All this shows that --up to isomorphism-- $\mZ$ and $[\lG,\lG]$ are Lie algebras of compact groups. We know from lemma \ref{lemLeibnitz} that the Lie algebra of $G\times H$ is $\lG\oplus\lH$. Thus, here, $\lG$ is the Lie algebra of the compact group $S^1\times\ldots\times S^1\times\Int(\lG)$.
\subdem{Reverse sense}
We consider a compact group $G$ and we have to see the its Lie algebra $\lG$ is compact. If $G$ is connected, $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(\lG)$. If $G$ is not connected, the Lie algebra of $G$ is $T_eG_0$ ($G_0$ is the identity component of $G$) where $G_0$ is connected and compact because closed in a compact.
\end{proof}
 
\begin{proposition}
Let $\lG$ be a real Lie algebra and $\mZ$, the center of $\lG$. We consider $\lK$, a compactly embedded in $\lG$. If $\lK\cap\mZ=\{0\}$ then the Killing form of $\lG$ is strictly negative definite on $\lK$.
\label{prop:K_Z_Killing}
\end{proposition}

\begin{proof}
Let $B$ be the Killing form on $\lG$ and $K$ the analytic subgroup of $\Int(\lG)$ whose Lie algebra is $\ad_{\lG}(\lK)$. By assumption, $K$ is a compact Lie subgroup of $\GL(\lG)$. Then there exists a quadratic form on $\lG$ invariant under $K$, and a basis in which the endomorphisms $\ad_{\lG}(T)$ for $T\in\lK$ are skew-symmetric because the matrices of $K$ are orthogonal. If the matrix of $\ad T$ is $(a_{ij})$, then
\begin{equation}
   B(T,T)=\sum_{ij}a_{ij}(T)a_{ji}(T)
         =-\sum_{ij}a_{ij}^2(T)\leq 0,
\end{equation}
and the equality hold only if $\ad T=0$ i.e. if $T\in\mZ$. From the assumptions, $\lK\cap\mZ=\{0\}$; then $B(T,T)=0$ if and only if $T=0$. 
\end{proof}

\subsection{Lorentz algebra}
%---------------------------

\begin{lemma}[\cite{Schomblond_em}]     \label{LemCommsopqAlg}
The matrices of $\so(p,q)$ satisfy the definition relation
\begin{equation}
    M^t\eta+\eta M=0,
\end{equation}
and if $M^{ab}$ is the ``rotation'' in the place of directions $a$ and $b$ (i.e. a trigonometric or an hyperbolic rotation following that $a$ and $b$ are of the same type or not), then the action on $\eR^{(p,q)}$ is given by $(x')^{\mu}=(M^{ab})^{\mu}_{\nu}x^{\nu}$ with
\begin{equation}
    (M^{ab})^{\mu}_{\nu}=\eta^{a\mu}\delta^b_{\nu}-\eta^{b\mu}\delta^a_{\nu}.
\end{equation}
The commutation relations are given by
\begin{equation}
    [M^{ab},M^{cd}]=-\eta^{ac}M^{bd}+\eta^{ad}M^{bc}+\eta^{bc}M^{ad}-\eta^{bd}M^{ac}.
\end{equation}
Notice that $M^{ab}=-M^{ba}$. 
\end{lemma}

There is an other physical reason (which is in fact the same, but differently presented) justifying the study of the Clifford algebra. The quantum field theory need representation of the Lorentz algebra\footnote{When one think to real infinitesimal rotation matrices, the presence of $i$ seems not natural, but one redefines $J\to iJ$ for formalism reasons.}\index{lorentz!algebra}
\[
 [J^{\mu\nu},J^{\rho\sigma}]=i(\eta^{\nu\rho}J^{\mu\sigma}-\eta^{\mu\rho}J^{\nu\sigma}
 -\eta^{\nu\sigma}J^{\mu\rho}+\eta^{\mu\sigma}J^{\nu\rho}).
\]
A proof of these relations is given in lemma \ref{LemCommsopqAlg}. Dirac had a trick to find such $J$ matrices from a representation of the Clifford algebra. If we have $n\times n$ matrices $\gamma_{\mu}$ such that
\[
    \gamma^{\mu}\gamma^{\nu}+\gamma^{\nu}\gamma^{\mu}=2\eta^{\mu\nu}\mtu_{n\times n},
\]
a $n$-dimensional representation of the Lorenz algebra is obtained by
\[
    S^{\mu\nu}=\frac{i}{4}\left[\gamma^{\mu},\gamma^{\nu}\right].
\]

By a simple redefinition $J=iM$, one obtains 
\begin{equation}            \label{EqJJietaJcomm}
    [J,J]=i\eta J
\end{equation}
instead of $[M,M]=\eta M$, and the matrices $J$ are Hermitian. Here $\eta$ is the matrix $\eta=diag(\underbrace{+,\ldots,+}_{\text{$p$ times}},\underbrace{-,\ldots,-}_{\text{$q$ times}})$. As convention, we say that a direction corresponding to a \emph{positive} entry in the metric is a \emph{time} direction, while the spatial directions are negative. That corresponds to the convention of page \pageref{PgDefsGenre} to say that a \emph{time-like} vector has positive norm.

\section{Clifford algebra}
%++++++++++++++++++++++++++

\subsection{Definition and universal problem}
%------------------------------------------------------


\begin{theorem}\index{Clifford!algebra!Universal property of}
Let $E$ be an unital associative algebra and $\dpt{j}{V}{E}$ a linear map such that
\begin{equation}
    j(v)\cdot j(v)=q(v)1.        \label{102r1}
\end{equation}
Then we have an unique extension of $j$ to a homomorphism $\dpt{\tilde\jmath}{\Cliff(V,q)}{E}$. Moreover, $\Cliff(V,q)$ is the unique associative algebra which have this property for all such~$E$.
\[
\xymatrix{
    \Cliff(V,q) \ar@{^{(}->}[d]_{\displaystyle i} \ar[rd]^{\displaystyle\tilde{j}} &  \\
    V \ar[r]_{\displaystyle j} & D
  }
\]
\label{tho_Cliffunif}
\end{theorem}
This theorem can be seen as a definition of $\Cliff(V,q)$.

\begin{proof}
The proof shall belongs two parts: the first one will show how to extend $j$ and why it is unique, and the second one will prove the unicity of $\Cliff(V,q)$.

We begin by define the extension of $j$. First note that any linear map $\dpt{f}{V}{E}$ can be extended to an algebra homomorphism $\dpt{\overline{f}}{T(V)}{E}$ in only one way. Indeed, the homomorphism condition require that $\overline{f}(v\otimes w)=f(v)\cdot f(w)$.  The whole map $\overline{f}$ is then well defined by the data of $f$ alone.

As far as the map $j$ is concerned, we have the relation \eqref{102r1} which says that $\overline{j}(\mI)=0$. Indeed,
\begin{equation}
 \ovj(v\otimes v-q(v)\cdot(1))=\ovj(v)\cdot\ovj(v)-q(v)\ovj(1)
                              =j(v)\cdot j(v)-q(v)1
                              =0.
\end{equation}
Thus $\dpt{\ovj}{T(V)}{E}$ is a class map for $\mI$, and we can descent $\ovj$ from $T(V)$ to $\Cliff(V,q)$, We define
$\dpt{\tilde\jmath}{\Cliff(V,q)}{E}$ by
\begin{equation}
         \tilde\jmath[x]=\ovj(x)
\end{equation}
where $[x]$ is the class of $x$. That's for the existence part.

The unicity is clear: $f_1=f_2$ on $V$ implies that $\overline{f_1}=\overline{f_2}$ on $T(V)$. Thus $\tilde{f_1}=\tilde{f_2}$ on $\Cliff(V,q)$.

We turn now our attention to the unicity of $\Cliff(C,q)$. Let $D$ be an unital associative algebra such that
\begin{enumerate}
\item $V\subset D$,
\item For any unital associative algebra $E$ and for any $\dpt{f}{D}{E}$ such that $f(v)\cdot f(v)=-q(v)1$, there exists only one homomorphic map $\dpt{\tilde{f}}{D}{E}$ which extend $f$.
\end{enumerate}
We should find a homomorphic map $\dpt{\tilde{k}}{D}{\Cliff(V,q)}$. Let $i$ be the canonical injection $\dpt{i}{V}{D}$. Clearly, we have a homomorphism $V\rightarrow i(V)$. Now, as a space $E$, we can take $\Cliff(V,q)$; $i$ can be seen as a linear map $\dpt{i}{V}{\Cliff(V,q)}$ such that $i(v)\cdot i(v)=q(v)1$. The assumptions say that $i$ can be extended (in only one way) to a homomorphic map $\dpt{\tilde{i}}{D}{\Cliff(V,q)}$.

The Clifford algebra is thus unique up to a homomorphism.

\end{proof}

What we proved is the following: if for any $E$ and for any $\dpt{j}{V}{E}$ such that $j(v)\cdot j(v)=q(v)1$, there exist an unique $\dpt{\tilde{j}}{D}{E}$ which extend $j$, then $D=\Cliff(V,q)$ up to a homomorphism. One ays that $\Cliff(V,q)$ solve an \defe{universal problem}{universal!problem}.


\section{The group \texorpdfstring{$SL(2,\eR)$}{SL2R} and its algebra}  \label{SecToolSL}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The study of \( \SL(2,\eR)\) and \( \gsl(2,\eC)\) is required before to go further in the general study because of proposition \ref{PropScalrooTsQ} that will reduce the study of genera Lie algebras into combinations of \( \gsl(2,\eC)\) algebras.

\subsection{Iwasawa decomposition}
%----------------------------------
\index{Iwasawa!decomposition!of $SL(2,\eR)$}

Let $G=\SL(2,\eR)$ the group of $2\times 2$ matrices with unit determinant. The Lie algebra $\lG=\gsl(2,\eR)$ is the algebra of matrices with vanishing trace:
\begin{equation}
 \lG =  \{ X\in\End(\eR^2)\tq \tr(X) = 0\} 
=\left\{ \begin{pmatrix}
x & y \\
z & -x
\end{pmatrix}\textrm{ with }x,y,z\in\eR  \right\}. 
\end{equation}
The following elements will be intensively used:
\begin{equation}    \label{EqsXPdnZlG}
H=\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
,\quad
  E=\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
,\quad
 F=\begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix},
\quad
T=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}
\end{equation}
where $T=E-F$ has been introduced for later convenience. The commutators are
\begin{subequations}\label{EqTableSLdR}
\begin{align}  
  [H,E]&=2E &[T,H]&=-2T  \\
  [H,F]&=-2F    &[T,E]&=H   \\
  [E,F]&=H  &[T,F]&=H.
\end{align}
\end{subequations}
The exponentials can be easily computed and the result is
\begin{align}               \label{EqExpMatrsSLdeuxR}
 e^{tH}=
\begin{pmatrix}
   e^{t}    &   0   \\ 
  0 &    e^{-t} 
\end{pmatrix},
&&
 e^{tE}=
\begin{pmatrix}
  1 &   t   \\ 
  0 &   1   
\end{pmatrix},
&&
 e^{tF}=
\begin{pmatrix}
  1 &   0   \\ 
  t &   1   
\end{pmatrix}.
\end{align}
Notice that the sets $\{ H,E,F \}$, $\{ H,E,F \}$ and $\{ H,E+F,T \}$ are basis. A Cartan involution is given by $\theta(X)=-X^t$, and the corresponding Cartan decomposition is
\begin{align}
   \lK&=\Span\{ T \},
&\lP&=\Span\{ H,E+F \}.
\end{align}
Indeed, we are in a matrix algebra, then $\tr(XY)$ is proportional to $\tr(\ad X\circ \ad Y)$.
 In order to see that $\theta$ is a Cartan involution, we have to prove that $B|_{\lK\times\lK}$ is negative definite and $B|_{\lP\times\lP}$ positive. It is true because for $X\in\lK$,
\[
    \tr(\ad X\circ \ad X)=\tr(XX)=\tr\begin{pmatrix}
-x^2 & 0 \\
0 & -x^2
\end{pmatrix}<0,
\]
and for $Y\in\lP$,
\[
    \tr(YY)=\tr\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix}\begin{pmatrix}
x & y \\
y & -x
\end{pmatrix} =\tr\begin{pmatrix}
x^2+y^2 & 0 \\
0 & x^2+y^2
\end{pmatrix} >0.
\]

Up to some choices, the Iwasawa decomposition\label{pg_iwasldr} of the group $\SL(2,\eR)$ is given by the exponentiation of $\lA$, $\lN$ and~$\lK$
\begin{equation}
\begin{aligned}
  \lA&=\Span\{ H \}
&\lN&=\Span\{ E \}
&\lK&=\Span\{T\},
\end{aligned}
\end{equation}
so that
\begin{equation}\label{eq:expo_ANK}
A=\begin{pmatrix}
e^a & 0 \\
0 & e^{-a}
\end{pmatrix}\quad
N=\begin{pmatrix}
1 & l \\
0 & 1
\end{pmatrix}\quad
K=\begin{pmatrix}
\cos k & \sin k \\
-\sin k & \cos k
\end{pmatrix}.
\end{equation}

A common parametrization of $AN$ by $\eR^2$ is provided by
\begin{equation}   \label{EqParmalSL} 
(a,l)=
\begin{pmatrix}
  e^a&le^a\\
  0  &e^{-a}
\end{pmatrix}.
\end{equation}
One immediately has the following formula for the left action of $AN$ on itself:
\[
  L_{(a,l)}(a',l')=\begin{pmatrix}
e^{a+a'} & e^{a+a'}l'+e^{a-a'}l \\
0 & e^{-a-a'}
\end{pmatrix}=(a+a',l'+e^{-2a'}l).
\]
In this setting, the inverse is given by $(a,l)^{-1}=(-a,-l e^{2a})$.  

Let's give some formulas in $\SL(2,\eR)$. Using corollary \ref{Ad_e} and exponentiating commutation relations,
\begin{subequations}  \label{eq_eaHsldr}
\begin{align}
\Ad(e^{aH})E&=e^{2a}E,\\
\Ad(e^{aH})F&=e^{-2a}F,\\
\Ad( e^{aH})T&= e^{2a}E- e^{-2u}E+ e^{-2u}T\\
\Ad(e^{tE})H&=H-2tE,                            \label{eq_AdetE}\\
\Ad( e^{tE})T&=-tH+(t^2+1)E-E+T\\
\Ad( e^{tT})H&=\cos(2t)H+\sin(2t)(2E-T)\\
\Ad( e^{tT})E&=\frac{ 1 }{2}\Big( \sin(2t)H+\cos(2t)(2E-T)+T \Big)
\end{align}
\end{subequations}
where $z$ belongs to the center: $z=\pm\mtu$.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{A companion : \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

We can consider the Iwasawa decomposition which is $\theta$-conjugated to the $AN$ that we just saw. That decomposition is generated by
\begin{equation}
    \bar\lN=\begin{pmatrix}
    0   &   0   \\ 
    1   &   0
\end{pmatrix}.
\end{equation}
The exponentiation produces
\begin{equation}
    \bar N=\begin{pmatrix}
    1   &   0   \\ 
    t   &   1   
\end{pmatrix},
\end{equation}
and the Iwasawa group is given by
\begin{equation}        \label{EqGeneANbarSLdeuxR}
    A\bar N=\begin{pmatrix}
    e^a &   0   \\ 
    l e^{-a}    &    e^{-a} 
\end{pmatrix}.
\end{equation}

\subsection{Killing form}
%------------------------

In the basis $\{ H,E,T \}$, the adjoint operators are given by
\[
\ad H=\begin{pmatrix}
 0 & 0 &-2 \\
 0 & 0 &0 \\ 
 0 & 2 &2
\end{pmatrix},
\ad E=\begin{pmatrix}
 0 & 0 &0 \\
 0 & 0 &-1\\ 
-2 & 0&0
\end{pmatrix},\textrm{ and }
  \ad T=\begin{pmatrix}
 2 & 0 &0 \\
0 & 1 &0\\
-2 & 0&0
\end{pmatrix}.
\]
so that the Killing form can be computed directly from definition $B(X,Y)=\tr(\ad X,\ad Y)$. The result is
\begin{subequations}
\begin{align}
B(T,H)&=0  & B(H,H)&=8\\
B(T,E)&=-4 & B(E,E)&=0\\
B(H,E)&=0  &  B(T,T)&=-4.
\end{align}
\end{subequations}
Expressed in the basis $\{H,E,F\}$, the matrix of the Killing form reads
\begin{equation}
B=
\begin{pmatrix}
8&&\\
&&4\\
&4&
\end{pmatrix}
\end{equation} 
while, in the basis  $\{H,E+F,T\}$, we find
\begin{equation}   \label{EqBHEFTsldR}
B=
\begin{pmatrix}
8\\
&8\\
&&-8
\end{pmatrix}.
\end{equation}
The latter is the reason of the name of the vector $T$: the sign of its norm is different, so that $T$ is candidate to be a time-like direction.

\subsection{Abstract root space setting}
%---------------------------------------

Looking on the table \eqref{EqTableSLdR} from an abstract point of view, we see that $E$ and $F$ are eigenvectors of $\ad(H)$ with eigenvalues $2$ and $-2$. So $\lA=\lG_0=\eR H$; $\lG_2=\eR E$; and $\lG_{-2}=\eR F$. Using a more abstract notation, the table of $\SL(2,\eR)$ becomes
\begin{subequations}  \label{subeq_rootSLR}
\begin{align}
  [A_{0},A_{2}]&=2A_{2}\\
    [A_{0},A_{-2}]&=-2A_{-2}\\
    [A_{2},A_{-2}]&=A_{0}.
\end{align}
\end{subequations}

\subsection{Isomorphism}
%-----------------------

As pointed out in the chapter II, \S6 of \cite{Knapp_reprez}, the map (seen as a conjugation in $\SL(2,\eC)$)
\begin{equation}
    \begin{aligned}
        \psi\colon \SU(1,1)&\to \SL(2,\eR) \\ 
        U&\mapsto AUA^{-1} 
    \end{aligned}
\end{equation}
with $A=\begin{pmatrix}
1&i\\i&1
\end{pmatrix}$ is an isomorphism between $\SL(2,\eR)$ and $\SU(1,1)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                    \section{The group \texorpdfstring{$\SO(3)$}{SO3} and its Lie algebra}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SubSecTheGroupSotrois}

We follow \cite{WormerAngular} in which more proofs can be found.

\begin{proposition}
An element of $\SO(3)$ has exactly one eigenvector with eigenvalue $1$. That vector is the \defe{rotation axis}{axis!of rotation in $\SO(3)$}.
\end{proposition}

The generator of rotation around the axis $n$ (unit vector) is given by the matrix
\begin{equation}
\begin{pmatrix}
  0 &   -n_3    &   n_2\\ 
  n_3   &   0   &   -n_1\\ 
 -n_2   &   n_1 & 0   
\end{pmatrix}.
\end{equation}
That form results form the requirement that $Nr=n\times r$. If we denote by $R(n,\theta)$ the operator of rotation in $\eR^3$ by an angle $\theta$ around the axis $n$, one shows that
\begin{equation}
    R(b,\theta)=\mtu+\sin(\theta) N+\big(1-\cos(\theta)\big)N^2.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                    \subsection{Rotations of functions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Consider any function $f\colon \eR^3\to \eC$; we define the \defe{rotation operator}{rotation!on functions} $U(n,\theta)$\nomenclature{$U(n,\theta)$}{Rotation operator on functions} by
\begin{equation}        \label{EqRotFunSOtrois}
    \big( U(n\theta)f \big)(r)=f\big( R(n,\theta)^{-1}r \big).
\end{equation}
These operators form a group, and we have in particular that
\[ 
    U(n,\theta_1)U(n,\theta_2)=U(n,\theta_1+\theta_2).
\]
We are interested in \emph{infinitesimal} rotations, that is rotations of angle $d\theta$ for which $(d\theta)^2\ll d\theta$, or in other words, we are interested in a development of equation \eqref{EqRotFunSOtrois} restricted to linear terms in $\theta$. What one obtains is
\begin{equation}
    \big( U(n,d\theta)f \big)(r)=\big( (1-i d\theta\, n\cdot l)f\big)(r)
\end{equation}
where the operator $l$ is defined by
\begin{equation}
    l=-ir\times\nabla.
\end{equation}
Its components $l_i=-i\epsilon_{ijk}r_j\partial_k $ satisfy commutation relations
\begin{equation}    \label{EqAldllepsl}
    [l_i,l_j]=i\epsilon_{ijk}l_k.
\end{equation}
The operator $n\cdot l$ is refereed as the \defe{generator of infinitesimal rotations}{generator!of infinitesimal rotations}. One can derive an expression of $U(n,\theta)$ in terms of $n\cdot l$ by the following:
\[ 
    U(n,\theta+d\theta)f=U(n,\theta)U(n,d\theta)f=U(n,\theta)(1-id\theta\, n\cdot l)f,
\]
so that we have the differential equation
\begin{equation}
    \frac{ dU }{ d\theta }(n,\theta)=-iU(n,\theta)n\cdot l
\end{equation}
with the initial condition $U(n,0)=1$. The solution is
\begin{equation}
    U(n,\theta)= e^{-i\theta\, n\cdot l}.
\end{equation}

\subsection{Representations of \texorpdfstring{$\SO(3)$}{SO3}}\label{pg:reprez_SOt}
%//////////////////////////////////////////////////////////////////////

The group $\SO(3)$ is strongly linked with $SU(2)$ by the following property:
\begin{equation}        \label{EqSOSUeZ}
   \SO(3)=\frac{SU(2)}{\eZ_2}.
\end{equation}

\begin{lemma}
A representation $\rho_j$ of $SU(2)$ is a representation of $\SO(3)$ if and only if $\rho_j(X)=\id$ for any $X$ in the kernel of the homomorphism $SU(2)\to \SO(3)$, namely: $\rho_j(\pm\mtu)=\id$.
\label{lem:SO_3}
\end{lemma}

\begin{proof}
We consider $\dpt{\rho_j}{SU(2)}{\End{V_j}}$ and $\dpt{\psi}{SU(2)}{\SO(3)}$. The latter fulfils $\psi(\mtu)=\psi(-\mtu)=\mtu$, which is an important equation because it ensures us that the rest of the expressions are well defined with respect to the class representative.

If $\rho_j(-\mtu)=\mtu$, we define $\dpt{d_j}{\SO(3)}{\End{V}}$ by $d_j([x])=\rho_j(x)$ (check that this is well defined). With this, 
\[
  d_j([x])d_j([y])=\rho_j(x)\rho_j(y)=\rho_j(xy)=d_j([xy]).
\]

Now let us suppose that $d_j([x])=\rho_j(x)$ is a representation. Thus 
\[
  \rho_j(x)=d_j([x])=d_j([-x])=\rho_j(-x)=\rho_j(-\mtu)\rho_j(x),
\]
so $\rho_j(-\mtu)=\id_{V_j}$.

\end{proof}

Moreover, any representation of $\SO(3)$ comes from a representation $\tilde\rho$ of $SU(2)$ by setting $\tilde\rho(-\mtu)=\id$ and $\tilde\rho(x)=\rho([x])$.

Now, we research the representations of $SU(2)$ for which the matrix $-\mtu$ is represented by the identity operator. These will be representations of $\SO(3)$. The spin $j$ representations of $SU(2)$ is given by
\[
   \rho_j(X)\phi_{pq}(\xi)=\phi_{pq}(X^{-1}\xi).
\]
With $X=-\mtu$, this gives: $\phi_{pq}(-\xi)=(-1)^{p+q}\phi_{pq}(\xi)$. If we want it to be equal to $\phi_{pq}(\xi)$, we need $p+q=2j$ even. This is true if and only if $j\in\eN$.

\label{pg:reprez_SO3}The conclusion is that the irreducible representations of $\SO(3)$ are the integer spin irreducible representations of $\SU(2)$. Note that the non relativistic mechanics has $\SO(3)$ as group of space symmetry. Thus there are no hope to find any half integer spin in a non relativistic theory.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Representations of the algebra \texorpdfstring{$\gsu(2)=\so(3)$}{su2so3}}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecPJmtqrG}

If one knows a load of theory, it is possible to determine the irreducible representations of \( \so(3)\) in a very short way; This will be done in example \ref{ExHESKimc}. We are now going to determine the irreducible representations of \( \so(3)\) in a quite explicit way. 

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Ladder operators}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

From the relation \eqref{EqSOSUeZ}, we know that the study of $\gsu(2)$ and $\so(3)$ are the same. The algebra $\gsu(2)$ is the real algebra generated by the matrices of the form
$
\begin{pmatrix}
\alpha  &\beta\\
-\beta^*&-\alpha
\end{pmatrix}
$ with $\alpha$, $\beta\in\eC$. A convenient basis is given by
\begin{align}       \label{EqGenssudeux}
u_1&=\frac{ 1 }{2}
\begin{pmatrix}
  i &   0   \\
  0 &   -i  
\end{pmatrix},
&u_2&=
\frac{ 1 }{2}
\begin{pmatrix}
  0 &   1   \\ 
  -1    &   0   
\end{pmatrix},
&u_3&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   i   \\ 
  i &   0
\end{pmatrix}.
\end{align}
That algebra satisfies the commutation relations
\begin{equation}
    [u_i,u_j]=\epsilon_{ijk}u_k.
\end{equation}
The trick to build finite dimensional representations\index{representation!of $\gsu(2)$} of that algebra is common (see \cite{MQSenechal} for example). The first step is to perform a change of basis $J_k=iu_k$ that brings the algebra under the form (see section \ref{SubSecTheGroupSotrois} to understand why)
\begin{equation}        \label{EqAlgsuiepsijk}
    [J_i,J_j]=i\epsilon_{ijk}J_k.
\end{equation}
We are going to construct all the finite dimensional irreducible representations of the algebra \eqref{EqAlgsuiepsijk}. The key point of that new basis is that one can define the \defe{ladder operators}{ladder operators}
\begin{equation}
    J_{\pm}=J_1\pm iJ_2
\end{equation}
that have the property that 
\begin{equation}
    [J_3,J_{\pm}]=\pm J_{\pm}.
\end{equation}
Notice that for every $i$, we have $(J_i)^*=J_i$, so that $(L^{\pm})^*=L^{\mp}$. An other important property is that, defining $J^2=J_1^2+J_2^2+J_3^2$, we have
\begin{equation}
    [J_i,J^2]=0,
\end{equation}
which show that $J^2$ is a Casimir operator, and is thus by Schur's lemma a multiple of identity. Notice that we are using an abuse of notation between $J_i$ as element of $\gsu(2)$ and $J_i$ as the operator that represent $J_i$. In the first case, products like $J_iJ_j$ make no sense\footnote{In fact, one has to understand these products as elements of the universal enveloping algebra. What we are building is a reprensentation of that algebra, which, obviously, restricts to a representation of the algebra. When we use the Schur's lemma, in fact we invoke it in $\mU\big(\so(3)\big)$}, but it makes sense as operator composition.

The subalgebra $\{ J^2,J_3 \}$ being abelian, we can simultaneously diagonalise $J^2$ and $J_3$. Let $| m,\sigma \rangle $ be an orthonormal basis of the eigenspace of $J_3$ associated with the eigenvalue $m$. The index $\sigma$ is for a possible degenerateness to be studied later.  We have
\[ 
    J_3| m,\sigma \rangle =m| m,\sigma \rangle .
\]
Using the commutation relations between $J_3$ and the ladder operators, we have
\begin{equation}        \label{EqJtroisJpmmplusun}
    J_3J_{\pm}| m,\sigma \rangle =\big( \pm J_{\pm}+J_{\pm}J_3 \big)| m,\sigma \rangle =(m\pm 1)J_{\pm}| m,\sigma \rangle .
\end{equation}
Thus $J_{\pm}| m,\sigma \rangle $ is an eigenvector of $J_3$ with the eigenvalue $m\pm 1$, which means that $J_{\pm}| m,\sigma \rangle $ is a linear combination of the vectors $| m\pm 1,\sigma \rangle $ with different values of $\sigma$. This is the reason of the name of the \emph{ladder} operators: they raise and lower the eigenvalue of $J_3$.

We can now prove that one has to drop the index $\sigma$ because eigenvalues of $J_3$ cannot be degenerated. For, compute
\begin{equation}        \label{JpJmJcarrerelation}
    J_+J_-=(J_1+iJ_2)(J_1-iJ_2)=J^2-J_3^2+i[J_2,J_1]=J^2-J_3^2+J_3,
\end{equation}
so that
\[ 
    J_+J_-| m,\sigma \rangle =(\alpha-m^2+m)| m,\sigma \rangle 
\]
where $\alpha$ is defined by $J^2=\alpha\mtu$. That proves that the space generated by $| m,\sigma \rangle $ and the action of $J_3$, $J_+$ and $J_-$ is invariant under the representation, while one cannot obtain $\ket{m,\sigma'}$ by action of $J_{\pm}$ on $\ket{m,\sigma}$. Since we are looking for \emph{irreducible} representations, that space must actually be all the representation space. That rules out the possibility to have two different vectors $| m,\sigma_1 \rangle $ and $| m,\sigma_2 \rangle $.

The explicit matrix form of $J_{\pm}$ are:
\begin{align}
J_{+}&=
\begin{pmatrix}
0   &   0   &   0   &0  &\hdots\\
1   &   0   &   0   &0  &\hdots\\
0   &   1   &   0   &0  &\hdots\\
0   &   0   &   1   &0  &\hdots\\
\vdots  &   \vdots  &   \vdots  &\vdots &\ddots
\end{pmatrix},
&J_{-}&=
\begin{pmatrix}
0   &   1   &   0   &0  &\hdots\\
0   &   0   &   1   &0  &\hdots\\
0   &   0   &   0   &0  &\hdots\\
0   &   0   &   0   &1  &\hdots\\
\vdots  &   \vdots  &   \vdots  &\vdots &\ddots
\end{pmatrix},
\end{align}
Since we are searching for finite dimensional representations, there exists a maximal eigenvalue of $J_3$. Let us denote by $j$ that maximal eigenvalue and by $| j \rangle$ the corresponding eigenvector. The relation \eqref{EqJtroisJpmmplusun} shows that if $J_+| j \rangle\neq 0$, then $J_+| j \rangle$ is an eigenvector for $J_3$ with eigenvalue $j+1$, which contradicts maximality. Then we have $J_+| j \rangle=0$.

Since we know the action of $J_3$ and $J_+$ on $| j \rangle$, it is convenient to write $J^2$ in terms of these two operators. This is done in the same way as probing equation \eqref{JpJmJcarrerelation}:
\begin{equation}
    J^2=J_3^2+J_3+J_-J_+,
\end{equation}
so that
\begin{equation}        \label{EqJcarrejjplusun}
    J^2| j \rangle=j(j+1)| j \rangle.
\end{equation}
We know that $J^2=\alpha\mtu$ and that $\alpha$ is a characteristic of the representation. What equation \eqref{EqJcarrejjplusun} tells us is that the maximal eigenvalue of $J_3$ is related to $\alpha$ by $j(j+1)=\alpha$.

We are now able to determine the proportionality constant of relation $J_{\pm}| m \rangle\propto| m\pm 1 \rangle$. Since $(J_-)^*=J_+$, we have
\begin{equation}    \label{EqnormeJmoinm}
    \| J_-| m \rangle \|^2=\langle m| J_+J_- | m \rangle = j(j+1)-m^2+m.
\end{equation}
Then one has
\begin{subequations}    
    \begin{align}
        J_-| m \rangle  &=\sqrt{j(j+1)-m(m-1)}| m-1 \rangle,    \label{EqJmoinsmanglemmointun}      \\
        J_+| m \rangle  &=\sqrt{j(j+1)-m(m+1)}| m+1 \rangle.
    \end{align}
\end{subequations}
As expected, $J_-| -j \rangle=0$ and $J_+| j \rangle=0$. Notice that we avoid the possibility $J_-| m \rangle=-\sqrt{\cdots}| m-1 \rangle$ by a simple redefinition $| m-1 \rangle\to -| m-1 \rangle$.

Equation \eqref{EqnormeJmoinm} shows that the norm of $| m \rangle$ becomes negative for $m<-j$ and $m>j+1$. We conclude that the minimal eigenvalue of $J_3$ is $-j$. Since $| j \rangle$ has to be reached from $| -j \rangle$ by action of $J_+$, the difference $j-(-j)$ must be an integer. Thus $j\in\eN/2$. The number $j$ is the \defe{spin}{spin!of representation of $\so(3)$} of the representation.

Let us give the explicit example with spin one half.
When $j=\frac{ 1 }{2}$, the vector space is generated by the vectors $| 1/2 \rangle$ and $| -1/2 \rangle$, and the operators are given by
\begin{align}
    J_3&=\frac{ 1 }{2}
\begin{pmatrix}
  1 &   0   \\ 
  0 &   -1  
\end{pmatrix},
&J_-&=
\begin{pmatrix}
  0 &   0   \\ 
  1 &   0   
\end{pmatrix},
&J_+&=
\begin{pmatrix}
  0 &   1   \\ 
  0 &   0   
\end{pmatrix},
\end{align}
from which we deduce
\begin{align*}
J_1&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   1   \\ 
  1 &   0   
\end{pmatrix},
&J_2&=
\begin{pmatrix}
  0 &   -i  \\ 
  i &   0   
\end{pmatrix}.
\end{align*}
Notice that we have $J_i=\frac{ 1 }{2}\sigma_i$ with the \defe{Pauli matrices}{pauli matrices},
\begin{align}
\sigma_1&=
\begin{pmatrix}
  0 &   1   \\ 
  1 &   0   
\end{pmatrix},
&\sigma_2&=
\begin{pmatrix}
  0 &   -i  \\ 
  i &   0   
\end{pmatrix},
&\sigma_3&=
\begin{pmatrix}
  1 &   0   \\ 
  0 &   -1  
\end{pmatrix}.
\end{align}
These matrices fulfil the relation 
\begin{equation}
    \sigma_i\sigma_j=\delta_{ij}+i\epsilon_{ijk}\sigma_k.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsubsection{Weight vectors}  \label{subSubSecweightsotrois}
%---------------------------------------------------------------------------------------------------------------------------

The algebra $\so(3)$ does not contain abelian subalgebra of dimension bigger than one, so a Cartan subalgebra is generated by $J_3$. The unique (up to dilatation) element of $\hH^*$ is thus given by $\alpha(J_3)=1$. The relation $[J_z,J_{\pm}]$ provides the root spaces:
\begin{equation}
    \begin{aligned}
        \so(3)_1    &=\{ J_+ \}\\
        \so(3)_{-1} &=\{ J_- \},
    \end{aligned}
\end{equation}
thus $\lN^{\pm}$ is generated by $J_{\pm}$.

%---------------------------------------------------------------------------------------------------------------------------
                    \section{The complex algebra \texorpdfstring{$\protect\gsl(2,\eC)$}{sl2C} and its representations}
%---------------------------------------------------------------------------------------------------------------------------
\label{SecsldeuxCandrepres}

The book \cite{Kassel} contains the representations of \( \gsl(2,\eC)\).

The algebra $\gsl(2,\eC)$ is the complex algebra of complex $2\times 2$ matrices with vanishing trace. As generating matrices, one can take the elements $u_i$ of \eqref{EqGenssudeux} and complete them by
\begin{align*}
v_1&=\frac{ 1 }{2}
\begin{pmatrix}
  -1    &   0   \\ 
  0 &   1   
\end{pmatrix},
&v_2&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   i   \\ 
  -i    &   0   
\end{pmatrix},
&v_3&=\frac{ 1 }{2}
\begin{pmatrix}
  0 &   -1  \\ 
  -1    &   0   
\end{pmatrix}
\end{align*}
which satisfy the commutation relations
\begin{subequations}
\begin{align}
    [v_i,v_j]&=-\epsilon_{ikj}u_k\\
    [v_i,u_j]&=\epsilon_{ikj}v_k.
\end{align}
\end{subequations}

\begin{remark}
    This is not the algebra \( \gsl(2,\eC)\) used in physics. The latter is the \emph{four}-dimensional \emph{real} algebra of trace vanishing \( 2\times 2\) complex matrices. There is one more generator and the representation theory is different. Moreover the physics works with the \emph{group} instead of the \emph{algebra}.
\end{remark}

The change of basis
\begin{align}
    x_j&=\frac{ 1 }{2}(u_j+iv_j),   &y_j&=\frac{ 1 }{2}(u_j-iv_j)
\end{align}
provides the simplification
\begin{align}
[x_i,x_j]&=\epsilon_{ijk}x_k    &[y_i,y_j]&=\epsilon_{ijk}y_k   &[x_i,y_j]&=0,
\end{align}
so that, as algebras, we have the isomorphism
\begin{equation}
    \gsl(2,\eC)=\gsu(2)\oplus\gsu(2).
\end{equation}
Thus the representation theory of $\gsl(2,\eC)$ is determined by the one of $\gsu(2)$.


\index{representation!of $\gsl(2,\eC))$}
We follow the presentation of \cite{GpAlgLie_Faraut}. Consider the space $\mP_m$ of homogeneous polynomials of degree $m$ in two variables with complex coefficients. The dimension of $\mP_m$ is $m+1$ and we have the following representation of $\SL(2,\eC)$ thereon:
\begin{equation}
    \big( \pi_m(g)f \big)(u,v)=f\big( 
g
\begin{pmatrix}
u\\v
\end{pmatrix}
 \big)
=
f(au+bv,cu+dv)
\end{equation}
if $g=\begin{pmatrix}
  a &   b   \\ 
  c &   d   
\end{pmatrix}$. We are going to determine the corresponding representation $\rho_m$ of the Lie algebra $\gsl(2,\eC)$ as algebra over complex numbers. 

A basis of $\gsl(2,\eC)$ over $\eC$ is given by the matrices $\{ H,E,F \}$ given in equation \eqref{EqsXPdnZlG} and are subject to the commutation relations
\begin{subequations}    \label{subEqsSBhuAWx}
    \begin{align}
        [H,E]&=2E\\
        [H,F]&=-2F\\
        [E,F]&=H.
    \end{align}
\end{subequations}




Using the exponentiation \eqref{EqExpMatrsSLdeuxR}, we find
\[ 
    \big( \pi_m( e^{tH})f \big)(u,v)=f( e^{t}u, e^{-t}v),
\]
so that
\begin{equation}
    \big( \rho_m(H)f \big)(u,v)=u\frac{ \partial f }{ \partial u }-v\frac{ \partial f }{ \partial v }.
\end{equation}
In the same way, we find
\begin{equation}
    \big( \rho_m(E)f \big)(u,v)=\Dsdd{ \big( \pi_m( e^{tE})f \big)(u,v) }{t}{0}=v\frac{ \partial f }{ \partial u },
\end{equation}
and
\begin{equation}
     \big( \rho_m(F)f \big)(u,v)=u \frac{ \partial f }{ \partial v }.
\end{equation}
A natural basis of $\mP_m$ is given by the monomials $f_j(u,v)=u^jv^{m-j}$ with $j=0,\ldots,m$. The representation $\rho_m$ on this basis reads
\begin{equation}        \label{EqReprezgsldeuxC}
\begin{split}
    \rho_m(H)f_j&=(2j-m)f_j\\
    \rho_m(E)f_j&=(m-j)f_{j+1}\\
    \rho_m(F)f_j&=jf_{j-1}.
\end{split}
\end{equation}

\begin{proposition}     \label{ProprhomirredsldeuxC}
The representation $\rho_m$ is irreducible.
\end{proposition}

\begin{proof}
Let $W\neq\{ 0 \}$ be an invariant subspace of $\mP_m$. If $p\in W$, from invariance, $\rho_m(H)(p)\in W$. If $p$ is a linear combination of $\{ f_j \}_{j\in I}$ ($I\subseteq \{ 0,\ldots m \}$), then $\rho_m(H)p$ is still a linear combination $q$ of elements in the same set. Thus there exists a linear combination of $p$ and $\rho_m(H)p$ which is a linear combination of $\{ f_j \}_{j\in J}$ with $J\subset I$ (strict inclusion). Using the same trick with $q$ and $\rho_m(H)q$, we still reduce the number of basis elements. Proceeding in the same way at most $m$ times, we find that one of the $f_j$ belongs to $W$. From there, acting with $\rho_m(E)$ and $\rho_m(F)$, one generates the whole $\mP_m$. That proves that $W=\mP_m$ and thus that $\rho_m$ is irreducible. 
\end{proof}

\begin{theorem}[\label{GpAlgLie_Faraut}]
Every $\eC$-linear irreducible finite dimensional representation of $\gsl(2,\eC)$ is equivalent to one of the $\rho_m$.
\end{theorem}

These results will be proved also in the quantum case in theorems \ref{ThoVfintemofdsldcun} and \ref{ThoVfintemofdslddeux}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Cartan subalgebras in complex Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecCartaninComplex}
About Cartan algebra, one can read \cite{Dragan,Berndt,Hochschild,SamelsonNotesLieAlg}.

In this section $\lG$ will always denotes a complex finite dimensional Lie algebra. 

\begin{definition}\label{PgDefCentralisateur}
    When \( \lH\) is a subalgebra of \( \lG\), the \defe{centralizer}{centralizer} of \( \lH\) is the set\nomenclature[G]{$\mZ(\lH)$}{the centralizer of \( \lH\)}
    \begin{equation}
        \mZ(\lH)=\{x\in\lG\tq [x,\lH]\subset\lH\}.
    \end{equation}
    More generally if $\lG$ is a Lie algebra and if $\lA$, $\lB$ are two subset of $\lG$, the centraliser of $\lA$ in $\lB$ is
    \begin{equation}
        \mZ_{\lB}(\lA)=\{X\in\lB\tq [X,\lA]=0\}.
    \end{equation}
    If $\lA$ is a subalgebra of $\lG$, its \defe{normalizer}{normalizer} is 
    \begin{equation}
        \lN_{\lA}=\{X\in\lG\tq [X,\lA]\subset\lA\}.
    \end{equation}
    One can check that $\lA$ is an ideal in $\lN_{\lA}$.
\end{definition}

\begin{definition}
A subalgebra $\lH$ of a Lie algebra $\lG$ is a \defe{Cartan subalgebra}{Cartan!subalgebra} if it is nilpotent and if it is its own centralizer: $[x,\lH]\subset\lH$ implies $x\in\lH$.
\end{definition}

Our first task is to show that every Lie algebra has a Cartan algebra.

\begin{lemma}[Primary decomposition theorem]
    Let \( V\) be a complex vector space and \( A\colon V\to V\) be linear map. Then we have the direct sum decomposition
    \begin{equation}        \label{EqPrimDecomTho}
        V=\bigoplus_{\lambda\in\eC}V_{\lambda}(A)
    \end{equation}
    where \( V_{\lambda}(A)=\{ v\tq (A-\lambda\mtu)^nv=0\text{ for some \( n\in\eN\)} \} \)
\end{lemma}
This is the result that restricts ourself to \emph{complex} Lie algebras when proving that Cartan subalgebras exist. Notice that the sum in \eqref{EqPrimDecomTho} is reduced to the eigenvalues of \( A\) since \( \lG_{\lambda}(A)=0\) when \( \lambda\) is not an eigenvalue. Indeed if \( \big( A-\lambda\mtu \big)^nY=0\) then \( (A-\lambda\mtu)^{n-1}Y\) is an eigenvector for \( A\) with eigenvalue \( \lambda\).

For any \( \lambda\in\eC\) and \( X\in\lG\) we consider the space
\begin{equation}
    \lG_{\lambda}(X)=\{ Y\in\lG\tq \big( \ad(X)-\lambda\mtu \big)^nY=0\text{ for some \( n\)} \}.
\end{equation}
The primary decomposition theorem implies the decomposition
\begin{equation}        \label{EqDecomplGpRimDecombijk}
    \lG=\bigoplus_{\lambda}\lG_{\lambda}(X)
\end{equation}
for each \( X\in\lG\). 

A small useful formula : if \( u\) is a derivation of the Lie bracket and if \( [X,Y]\) is any bracket, then
\begin{equation}\label{EqWGujmeF}
    (u-\lambda\mtu)[X,Y]=\big[ (u-\lambda\mtu)X,Y \big]+[X,uY].
\end{equation}

\begin{lemma}   \label{LemVZzSnUW}
    For each \( X\in\lG\) and \( \lambda,\mu\in\eC\) we have
    \begin{equation}
        \big[ \lG_{\lambda}(X),\lG_{\mu}(X) \big]\subset\lG_{\lambda+\mu}(X).
    \end{equation}
\end{lemma}

\begin{proof}
    Let \( X_{\lambda}\in\lG_{\lambda}(X)\) and \( X_{\mu}\in\lG_{\mu}(X)\). Using the fact that \( \ad(X)\) is a derivation we have
    \begin{equation}
        \ad(X)[X_{\lambda},X_{\mu}]-(\lambda+\mu)[X_{\lambda},X_{\mu}]=\Big[ \big( \ad(X)-\mu\mtu \big)X_{\lambda},X_{\mu} \Big]+\Big[ X_{\lambda},\big( \ad(X)+\mu\mtu \big)X_{\mu} \Big]
    \end{equation}
    Let us show by induction the following equality for all \( n\) :
    \begin{equation}    \label{EqPIzsRhb}
        \big( \ad(X)-(\lambda+\mu)\mtu \big)^n[X_{\lambda},X_{\mu}]=\sum_{i=0}^{\infty}\binom{ n }{ i }\Big[ \big( \ad(X)-\lambda\mtu \big)^iX_{\lambda},\big( \ad(X)-\mu\mtu \big)^{n-i}X_{\mu} \Big].
    \end{equation}
    In order to prove that, it is sufficient to apply \( \big( \ad(X)-(\lambda+\mu)\mtu \big)\) to that equality and use the fact that \( \ad(X)\) is a derivation of the Lie bracket. Then apply formula \eqref{EqWGujmeF}.
    
The expression \eqref{EqPIzsRhb} vanishes when \( n\) is large enough.
\end{proof}

We say that \( X\) is \defe{regular}{regular} if \( \dim\lG_0(X)\) is the smallest with respect to the others \( \dim\lG_0(Y)\).

The following proposition shows that every complex Lie algebra has a Cartan Lie subalgebra.
\begin{proposition}
    If $X$ is regular in \( \lG\) then the subalgebra \( \lG_0(X)\) is Cartan.
\end{proposition}

\begin{proof}
    Since \( X\in\lG_0(X)\) we have \( \ad(X)\lG_{\lambda}(X)\subset\lG_{\lambda}(X)\). Thus we see \( \ad(X)\) as a linear operator on \( \lG_{\lambda}(X)\). The operator \( \ad(X)|_{\lG_{\lambda}(X)}\) is nonsingular\footnote{it means that \( \ad(Y)\) is invertible.} when \( \lambda\neq 0\). Indeed all the eigenvalues of \( \ad(X)\) on \( \lG_{\lambda}(X)\) are equal to \( \lambda\) because  
    \begin{equation}
        \big( \ad(X)-\mu\mtu \big)Y=0
    \end{equation}
    implies \( Y\in\lG_{\mu}(X)\). If \( Y\in\lG_{\lambda}(X)\) it only occurs when \( \mu=\lambda\) since the sum \eqref{EqPrimDecomTho} is direct.

    For each eigenvalue \( \lambda\) we have a neighborhood \( \mU_{\lambda}\) of $X$ in \( \lG_0(X)\) such that for all \( Y\in\mU_{\lambda}\), \( \ad(Y)\) is nonsingular on \( \lG_{\lambda}(X)\). We consider \( \mU=\bigcap_{\lambda}\mU_{\lambda}\) which is a non empty open set since the intersection is taken over the eigenvalues of \( \ad(X)\) that are in finite numbers.

    Let us prove that the restriction to \( \lG_0(X)\) of the linear operator \( \ad(Y)\) is nilpotent for each \( Y\in\mU\). First we have 
    \begin{equation}        \label{EqLgzsubsetlzY}
        \lG_0(Y)\subseteq\lG_0(X)
    \end{equation}
    because by construction \( \ad(Y)\) cannot be nilpotent on the other spaces \( \lG_{\lambda}(X)\). But by hypothesis the element \( X\) is regular, thus the inclusion \eqref{EqLgzsubsetlzY} cannot be strict. Thus \( \lG_0(X)\subset\lG_0(Y)\) which means that \( \ad(Y)\) is nilpotent on \( \lG_0(X)\).

    Now the fact for \( \ad(Y)\) to be nilpotent means the vanishing of a polynomial determined by the coefficients of the matrix of \( \ad(Y)\). Since this polynomial vanishes on the open set \( \mU\), it vanishes identically, so that \( \ad(Y)\) is nilpotent on \( \lG_0(X)\). It results that \( \lG_0(X)\) is a \( \ad\)-nilpotent algebra and the Engel's theorem \ref{tho:Engel} concludes that \( \lG_0(X)\) is nilpotent.

    We still have to prove that \( \lG_0(X)\) is its own centralizer. Since \( \lG_0(X)\) is a subalgebra we have the inclusion
    \begin{equation}
        \lG_0(X)\subseteq\mZ\big( \lG_0(X) \big).
    \end{equation}
    Let \( Z\in\mZ\big( \lG_0(X) \big)\). For each \( Y\in\lG_0(X)\) we have \( [Z,Y]\in\lG_0(X)\). In particular with \( Y=X\) we have \( \ad(X)Z\in\lG_0(X)\). Thus
    \begin{equation}
        \ad(X)^nZ=\ad(X)^{n-1}\underbrace{\ad(X)Z}_{\in\lG_0(X)}
    \end{equation}
    and there exists a \( n\) such that \( \ad(X)^{n-1}\ad(X)Z=0\).
\end{proof}

If \( \lG\) is a Lie algebra, the group of \defe{inner automorphism}{inner!automorphism} is the subgroup of \( \Aut(\lG)\) generated by the elements of the form \(  e^{\ad(X)}\) with \( X\in\lG\). This definition is motivated in the context of matrix groups by the fact that when \( g= e^{Y}\in G\) and \( X\in\lG\) we have
\begin{equation}
    gXg^{-1}= e^{\ad(Y)}X.
\end{equation}
\begin{example}
    If 
    \begin{equation}
        \begin{aligned}[]
            g=\begin{pmatrix}
                \cos(t)    &   \sin(t)    &   0    \\
                -\sin(t)    &   \cos(t)    &   0    \\
                0    &   0    &   1
            \end{pmatrix},&&X=\begin{pmatrix}
                0    &   a    &   b    \\
                -a    &   0    &   0    \\
                -b    &   0    &   0
            \end{pmatrix},
        \end{aligned}
    \end{equation}
    then one checks that \( g= e^{Y}\) with
    \begin{equation}
        Y=\begin{pmatrix}
              0  &  t     &   0    \\
            -t    &   0    &   0    \\
            0    &   0    &   0
        \end{pmatrix}
    \end{equation}
    and 
    \begin{equation}
        gXg^{-1}= e^{\ad(Y)}X=\begin{pmatrix}
            0    &   a    &   b\cos(t)    \\
            -a    &   0    &   -b\sin(t)    \\
            -b\cos(t)    &   b\sin(t)    &   0
        \end{pmatrix}.
    \end{equation}
\end{example}

\begin{theorem}
    The group of inner automorphisms of \( \lG\) acts transitively on the set of Cartan subalgebras. 
\end{theorem}

For a proof, see \cite{SerreSSAlgebres}. In particular they have all the same dimension and the definition of the \defe{rank}{rank!of a complex Lie algebra} as the dimension of its Cartan algebra make sense. In \cite{SerreSSAlgebres} we have a more abstract definition of the rank, see page III-2.

\begin{proposition}     \label{PropCartanLzXtjs}
    If \( \lH\) is a Cartan subalgebra of the complex Lie algebra \( \lG\), there exists a regular element \( X\in\lG\) such that \( \lH=\lG_0(X)\).
\end{proposition}

For a proof, see \cite{SerreSSAlgebres}.

\begin{proposition}\label{prop:Cartan_max_nil}
A Cartan subalgebra is a maximal nilpotent subalgebra.
\end{proposition}

\begin{proof}
Let $\lH$ be a Cartan subalgebra of $\lG$ and $\lN$, a nilpotent algebra which contains $\lH$. Let $\{X_1,\ldots,X_n\}$ be a basis of $\lG$ chosen in such a way that the $p$ first vectors form a basis of $\lH$ while the $r$ first, a basis of $\lN$ ($r>p$ of course). As notational convention, the subscript $i,j$ are related to $\lH$ and $u,t$ to $\lN\ominus\lH$.

Let us first suppose $\dim\lN=\dim\lH+1$ and let $X_u$ be the basis vector of $\lN$ which is not in $\lH$. Since $\lH$ is Cartan, we can find $X_i\in\lH$ such that $Y=[X_u,X_i]\notin\lH$. Then $Y$ has a $X_u$-component and this contradict the fact that $\ad X_i$ is nilpotent.

The next case is $\lN=\lH\oplus X_u\oplus X_t$. In this case we can find a $X_i\in\lH$ such that $Y=[X_u,X_i]\notin\lH$. The fact to be nilpotent makes that $Y$ has no $X_u$-component, so that it has a $X_t$-component. Now it is clear that for any $X_j\in\lH$, $[Y,X_j]$ still has no $X_u$-component (because $(\ad X_i\circ\ad X_j)$ has to be nilpotent), but has also no $X_t$-component. Then for any $X\in\lH$, $[Y,X]\in\lH$ with $Y\notin\lH$. There is a contradiction.

Now the step to the general case is easy: if $\dim\lN=\dim\lH+m$, we  consider $X_1,\ldots,X_m\in\lH$ and $A=(\ad X_1\circ\ad X_m)X_u$. This is not in $\lH$ although $[A,X]\in\lH$ for any $X\in\lH$.
\end{proof}


\begin{proposition}
    If \( \lG\) is a semisimple Lie algebra, a subalgebra \( \lH\) is Cartan if and only if the two following conditions are satisfied:
    \begin{enumerate}
        \item
            \( \lH\) is a maximal abelian subalgebra
        \item
            the endomorphism \( \ad(H)\) is diagonalizable for every \( H\in\lH\).
    \end{enumerate}
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Root spaces in semisimple complex Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecRootcomplexss}
In this section we particularize ourself to complex semisimple Lie algebras. A very good reference about complex semisimple algebras including the reconstruction \emph{via} the Cartan matrix and Chevalley-Weyl basis is \cite{SerreSSAlgebres}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Introduction and notations}
%---------------------------------------------------------------------------------------------------------------------------

Real and complex Lia algebras deserve quite different treatment with root space. We review here the main steps in both cases, emphasising the differences. We restrict ourself to semisimple Lie algebras. See \cite{Wisser}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Complex Lie algebras}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

If \( \lG\) is a complex semisimple Lie algebra, we choose a Cartan subalgebra \( \lH\) and the root spaces are given by
\begin{equation}
    \lG_{\alpha}=\{ X\in\lG\tq [H,X]=\alpha(H)X \forall H\in\lH \}.
\end{equation}
The dimension of \( \lH\) is the rank of \( \lG\). Then the root space decomposition reads
\begin{equation}
    \lG=\lH\oplus\bigoplus_{\alpha\in\Phi}\lG_{\alpha}
\end{equation}
where \( \Phi\) is the set of roots.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Real Lie algebras}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

If \( \lG\) is a real semisimple Lie algebra we consider a Cartan involution and the Cartan decomposition \( \lG=\lK\oplus\lP\). Then we choose a maximally abelian subalgebra \( \lA\) in \( \lP\) and we define
\begin{equation}
    \lG_{\lambda}=\{ X\in\lG\tq [J,X]=\alpha(J)X \forall J\in\lA \}.
\end{equation}
The rank of \( \lG\) is the dimension of \( \lA\). The root space decomposition then reads
\begin{equation}
    \lG=\lG_0\oplus\bigoplus_{\lambda\in\Sigma}\lG_{\lambda}
\end{equation}
where \( \Sigma\) is the set of \( \lambda\in\lA^*\) such that \( \lambda\neq 0\) and \( \lG_{\lambda}\neq 0\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Notations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{SubsecNotationRootsDel}

We summarize the notations that will be used later. Let \( \lH\) be a Cartan algebra in the complex semisimple Lie algebra \( \lG\). An element \( \alpha\in\lH^*\) is a root if the space
\begin{equation}
    \lG_{\alpha}=\{ X\in\lG\tq \ad(H)X=\alpha(H)x,\forall H\in\lH \}
\end{equation}
is non empty.

\begin{enumerate}
    \item
        \( \Phi\) is the set of all the roots. We consider an ordering notion on \( \Phi\) and \( \Phi^+=\Pi\) is the set of positive roots.
    \item
        An element in \( \Phi^+\) is simple if it cannot be written as the sum of two positive roots.
    \item
        \( \Delta\) is the set of simple roots\footnote{The symbol \( \Delta\) has not a fixed signification in the literature. As example, in \cite{Cornwell} the symbol \( \Delta\) is the set of roots while in \cite{SternLieAlgebra} it denotes the set of simple roots.}. The simple roots are denoted by \( \{ \alpha_1,\ldots,\alpha_l \}\).
\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Root spaces}
%---------------------------------------------------------------------------------------------------------------------------

We are considering a complex semisimple Lie algebra \( \lG\) with a Cartan subalgebra \( \lH\).

\begin{definition}      \label{DefRootSpace}
    For each \( \alpha\in\lH^*\) we define
    \begin{equation}            \label{eq:lG_alpha_nss}
        \lG_{\alpha}=\{  x\in\lG\tq\forall h\in\lH, \big(\ad h-\alpha(h)\big)^nx=0\text{ for some $n\in\eN$}    \}.
    \end{equation}
    If \( \lG_{\alpha}\) is not reduced to \( 0\), we say that \( \alpha\) is a \defe{root}{root} and \( \lG_{\alpha}\) is a \defe{root space}{root!space}.
\end{definition}
Corollary \ref{CorCoolWrlGbalpha} will provide an easier formula for the root spaces when the algebra \( \lG\) is complex and semisimple.
\begin{theorem}     \label{TholGCartalphaplusbeta}
Let $\lG$ be a complex Lie algebra with Cartan subalgebra $\lH$. If $\alpha,\beta\in\lG^*$ then
\begin{enumerate}
    \item   \label{ItemTholGCartalphaplusbetai}
    $[\lG_{\alpha},\lG_{\beta}]=\lG_{\alpha+\beta}$,
\item $\lG_0=\lH$.
\end{enumerate}
\label{prop:deux_racine}
\end{theorem}

\begin{proof}
For $z\in\lH$ and $x$, $y\in\lG$ we have
\begin{equation}
\big( \ad z-(\alpha+\beta)(z) \big)[x,y]=[ (\ad z-\alpha(z))x,y ]+[ x,(\ad z-\beta(z))y ].
\end{equation}
Using the same induction as in the proof of lemma \ref{LemVZzSnUW} we show that
\begin{equation}
\big( \ad z-(\alpha+\beta)(z) \big)^n[x,y]=\sum_k \binom{k}{n}
\left[
(\ad z-\alpha(z))^k(x),(\ad z-\beta(z))^{n-k}(y)
\right].
\end{equation}
This formula shows that $[\lG_{\alpha},\lG_{\beta}]\subset\lG_{\alpha+\beta}$. Indeed let $x\in\lG_{\alpha}$, $y\in\lG_{\beta}$ and $n$ be large enough,
\begin{equation}
    \left( \ad z-(\alpha+\beta)(z) \right)^n[x,y]=0.
\end{equation}

    Now we turn our attention to the second part. Let us apply the Lie theorem \ref{tho:Lie_Vu} to the action of \( \lG\) on the quotient \( \lG_0/\lH\). There exists \( [X_0]\in\lG_0/\lH\) such that \( h[X_0]=\lambda(h)[X_0]\) where the bracket stand for the class. Since \( \lH\) is nilpotent on \( \lG_0\) we have \( \lambda=0\) identically. Looking outside the class, the existence of a non vanishing \( [X_0]\in\lG/\lH\) such that \( h[X_0]=0\) means that there exists \( X_0\in\lG_0\setminus\lH\) such that \( [h,X_0]\in\lH\) for every \( h\in\lH\). This contradicts the fact that \( \lH\) is its own centralizer.
\end{proof}

\begin{proposition}
    The complex Lie algebra decomposes into the root spaces as
    \begin{equation}
        \lG=\bigoplus_{\alpha\in\lH^*}\lG_{\alpha}.
    \end{equation}
\end{proposition}

\begin{proof}
    Let \( H\in\lH\). We consider the primary decomposition \eqref{EqDecomplGpRimDecombijk} with respect to the operator \( \ad(H)\):
    \begin{equation}
        \lG=\bigoplus_{\lambda}\lG_{\lambda}(H).
    \end{equation}
    If \( H'\in\lH\) the operator \( \ad(H')\) acts the space \( \lG_{\lambda}(H)\) because \( H'\in\lG_0(H)\) so that
    \begin{equation}
        [H',\lG_{\lambda}(H)]\subset\lG_{\lambda}(H).
    \end{equation}
    Thus we can write the primary decomposition of \( \lG_{\lambda}(H)\) with respect to the operator \( \ad(H')\) knowing that
    \begin{equation}
        \big( \lG_{\lambda}(H) \big)_{\mu}(H')=\{ X\in\lG_{\lambda}(H)\tq\big( \ad(H')-\mu \big)^nX=0 \}=\lG_{\lambda}(H)\cap\lG_{\mu}(H').
    \end{equation}
    What we get is the decomposition
    \begin{equation}
        \lG=\bigoplus_{\lambda}\bigoplus_{\mu}\lG_{\lambda}(H)\cap\lG_{\mu}(H').
    \end{equation}
    We continue the decomposition with \( H'',H''',\ldots\) until each \( \ad(H)\) with \( H\in\lH\) has only one eigenvalue on each of the summand of the decomposition
    \begin{equation}
        \lG=\bigoplus_{\lambda_1,\ldots,\lambda_l}\lG_{\lambda_1}(H_1)\cap\ldots\cap\lG_{\lambda_l}(H_l).
    \end{equation}
    For each \( l\)-uple \( (\lambda_1,\ldots,\lambda_l)\), the eigenvalue of \( H_i\) on \( \lG_{\lambda_1}\cap\ldots\cap\lG_{\lambda_l}\) is \( \lambda_i\). Thus we can see \( \lambda\) as a \( 1\)-form on \( \lH\) and write
    \begin{equation}        \label{EqdirectumlGRoots}
        \lG=\bigoplus_{\lambda}\lG_{\lambda}
    \end{equation}
    with
    \begin{equation}
        \lG_{\lambda}=\{ X\in\lG\tq\big( \ad(H)-\lambda(H) \big)^nX=0 \}. 
    \end{equation}
\end{proof}

\begin{corollary}\label{cor:Bxy_zero}
    If $X_{\alpha}\in\lG_{\alpha}$ and $X_{\beta}\in\lG_{\beta}$ with $\alpha+\beta\neq 0$, then $B(X_{\alpha},X_{\beta})=0$.
\end{corollary}

\begin{proof}
    From the second point of proposition \ref{prop:deux_racine}, we have $\dpt{\ad X_{\alpha}\circ\ad X_{\beta}}{\lG_{\mu}}{\lG_{\mu+\alpha+\beta}}$. If $\alpha+\beta\neq 0$, the fact that the sum \eqref{EqdirectumlGRoots} is direct makes the trace of $\ad X_{\alpha}\circ\ad X_{\beta}$ zero.
\end{proof}


Since \( \lG\) is semisimple, the restriction of the Killing form on \( \lH\) is nondegenerate\footnote{Because the Killing form is zero on each space \( \lG_{\alpha}\) with \( \alpha\neq 0\).}. Thus we can introduce, for each linear function $\phi\colon \lH\to \eC$, the unique element $t_{\phi}\in\lH$ such that 
\begin{equation}
    \phi(h)=B(t_{\phi},h) 
\end{equation}
for every $h\in\lH$. \nomenclature[G]{\( t_{\alpha}\)}{a basis of \( \lH\)} This element is nothing else that the dual \( \phi^*\) with respect to the Killing form. Indeed
\begin{equation}
    t_{\phi}^*(h)=B(t_{\phi},h)=\phi(h),
\end{equation}
so that \( t_{\phi}^*=\phi\). Incidentally, this proves that when \( \phi\) runs over a basis of \( \lH^*\), the vector \( t_{\phi}\) runs over a basis of \( \lH\). The space $\lH^*$ is endowed with an inner product defined by\nomenclature{$(\alpha,\beta)$}{Inner product on the dual $\lH^*$ of a Cartan algebra}\nomenclature[G]{\( (\alpha,\beta)\)}{inner product on the dual \( \lH^*\).}
\begin{equation}        \label{EqDefInnprHestrar}
    (\alpha,\beta) = B(t_{\alpha},t_{\beta})=\beta(t_{\alpha})=\alpha(t_{\beta}).
\end{equation}

\begin{lemma}       \label{LemXYBXYtalpha}\label{Propoxalphaymoinaalpha}
    If \( X\in\lG_{\alpha}\) and \( Y\in\lG_{-\alpha}\), then
    \begin{equation}
        [X,Y]=B(X,Y)t_{\alpha}.
    \end{equation}
\end{lemma}

\begin{proof}
    By theorem \ref{TholGCartalphaplusbeta}\ref{ItemTholGCartalphaplusbetai}, $[X,Y]\in\lG_0=\lH$. Now we consider $h\in\lH$ and the invariance formula \eqref{eq:Killing_invariant}. We find:
    \begin{equation}
        B\big( h,[X,Y] \big)=-B\big( [X,h],Y \big)=\alpha(h)B(X,Y)=B(h,t_{\alpha})B(X,Y)=B\big(h,B(X,Y)t_{\alpha}\big).
    \end{equation}
    The lemma is proven since it is true for any $h\in\lH$ and $B$ is nondegenerate on $\lH$. 
\end{proof}

The elements \( t_{\alpha}\) allow to introduce an inner product on \( \lH^*\) and hence on the roots by defining
\begin{equation}
    (\alpha,\beta)=B(t_{\alpha},t_{\beta}).
\end{equation}

\begin{lemma}       \label{Leminnerabequaaggb}
    If \( \alpha\) and \( \beta\) are roots we have the formula
    \begin{equation}
        (\alpha,\beta)=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})(\alpha,\gamma)(\beta,\gamma).
    \end{equation}
\end{lemma}

\begin{proof}
    We consider for \( \lG\) a basis in which all the elements are part of one of the root spaces and we look at the endomorphism \( \ad(t_{\alpha})\) of \( \lG\). This is diagonal and has zeros on the entries corresponding to \( \lH\). The other entries on the diagonal are of the form \( \gamma(t_{\alpha})\). Thus
    \begin{equation}
        B(t_{\alpha},t_{\beta})=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})\gamma(t_{\alpha})\gamma(t_{\beta}).
    \end{equation}
    Thus we have \( (\alpha,\beta)=B(t_{\alpha},t_{\beta})=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})(\alpha,\gamma)(\beta,\gamma)\).
\end{proof}

\begin{proposition}[\cite{Cornwell}]     \label{PropScalrooTsQ}
    Let \( \alpha\) and \( \beta\) be roots. We have
    \begin{enumerate}
        \item
            \( (\alpha,\beta)\in\eQ\),
        \item
            \( (\alpha,\alpha)\geq 0\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Let \( \alpha,\beta\in\Phi\) and consider the space
    \begin{equation}
        V=\bigoplus_{m\in\eZ}\lG_{\beta+m\alpha}.
    \end{equation}
    If \( X_{\alpha}\in\lG_{\alpha}\) and \( X_{-\alpha}\in\lG_{-\alpha}\) with \( [X_{\alpha},X_{-\alpha}]=t_{\alpha}\) we have, for all \( v\in V\), 
    \begin{subequations}
        \begin{align}
            [X_{\alpha},v]&\in V\\
            [X_{-\alpha},v]&\in V\\
            [t_{\alpha},v]&\in V.
        \end{align}
    \end{subequations}
    Thus we can consider the restrictions to \( V\) of the operators \( \ad(X_{\alpha})\), \( \ad(X_{-\alpha})\) and \( \ad(t_{\alpha})\). Since \( \ad\) is an homomorphism we have, as operator on \( V\), 
    \begin{equation}
        \ad(t_{\alpha})=\big[ \ad(X_{\alpha}),\ad(X_{-\alpha}) \big],
    \end{equation}
    and then \( \tr\big( \ad(t_{\alpha})|_V \big)=0\). 

    Let us compute that trace on the basis \( \{ v_k^{(i)} \}\) where \( v_k^{(i)}\in\lG_{\beta+k\alpha}\). Since
    \begin{equation}
        \ad(t_{\alpha})v_k^{(i)}=(\beta+k\alpha)(t_{\alpha})v_k^{(i)}
    \end{equation}
    we have
    \begin{subequations}
        \begin{align}
            0&=\tr\big( \ad(t_{\alpha})|_V \big)\\
            &=\sum_{k\in\eZ}\dim\lG_{\beta+k\alpha}(\beta+k\alpha)(t_{\alpha})\\
            &=\sum_{k\in\eZ}\dim_{\beta+k\alpha}\big( (\alpha,\beta)+(\alpha,\alpha) \big)
        \end{align}
    \end{subequations}
    and
    \begin{equation}        \label{EqunderbAabmaaB}
        \underbrace{\left( \sum_{k\in\eZ}\dim\lG_{\beta+k\alpha} \right)}_{A\in\eN}(\alpha,\beta)=-(\alpha,\alpha)\underbrace{\left( \sum_{k\in\eZ}k\dim\lG_{\beta+k\alpha} \right)}_{B\in\eZ}.
    \end{equation}
    If \( (\alpha,\alpha)=0\) then we have \( (\beta,\alpha)=0\) for every \( \beta\in\Phi\), hence \( B(t_{\alpha},t_{\beta})=0\) which contradicts non degeneracy of the Killing form. We conclude that \( (\alpha,\alpha)\neq 0\). By the formula of lemma \ref{Leminnerabequaaggb} we get
    \begin{equation}
        (\alpha,\alpha)=\sum_{\beta\in\Phi}\dim\lG_{\beta}(\alpha,\beta)^2.
    \end{equation}
    Replacing in that formula the value of \( (\alpha,\beta)\) taken from formula \eqref{EqunderbAabmaaB} we found
    \begin{equation}
        (\alpha,\alpha)=\sum_{\beta\in\Phi}\dim\lG_{\beta}\frac{ B^2 }{ A^2 }(\alpha,\alpha)^2
    \end{equation}
    and then \( (\alpha,\alpha)\in\eQ^+\). The fact that \( (\alpha,\beta)\) is rational follows.

    Notice that the sign of \( B\) is not guaranteed because it's not sure because we do not know whether there are more positive or negative terms in the sum of the right hand side of \eqref{EqunderbAabmaaB}.
\end{proof}

\begin{proposition}
    Let \( \alpha\) be a root of the complex semisimple Lie algebra \( \lG\). Then 
    \begin{enumerate}
        \item
            \( \dim\lG_{\alpha}=1\), 
        \item
            the only integer multiple of \( \alpha\) to be roots are \( \pm\alpha\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Let \( X_{\alpha}\in\lG_{\alpha}\) and consider the vector space
    \begin{equation}
        V=\eC t_{\alpha}\oplus\eC X_{\alpha}\oplus\bigoplus_{m<0}\lG_{m\alpha}.
    \end{equation}
    Let \( y\in\lG_{-\alpha}\) be chosen in such a way that \( [X_{\alpha},y]=t_{\alpha}\); by lemma \ref{LemXYBXYtalpha} this is only a matter of normalization. The space \( V\) is invariant under \( \ad(X_{\alpha})\) and \( \ad(y)\). Indeed
    \begin{enumerate}
        \item
            \( \ad(X_{\alpha})t_{\alpha}=-\alpha(t_{\alpha})X_{\alpha}\in\eC X_{\alpha}\);
        \item
            \( \ad(X_{\alpha})X_{\alpha}=0\);
        \item
            \( \ad(X_{\alpha})\lG_{m\alpha}\subset\lG_{(m+1)\alpha}\); if \( m<-1\), \( (m+1)<0\), while if \( m=-1\) we know that the commutator \( [X_{\alpha},\lG_{-\alpha}]\) is included in \( \eC t_{\alpha}\in V\);
        \item
            \( \ad(y)t_{\alpha}\in\lG_{-\alpha}\)
        \item
            \( \ad(y)X_{\alpha}=-t_{\alpha}\) by definition;
        \item
            \( \ad(y)\lG_{m\alpha}\subset\lG_{(m-1)\alpha}\).
    \end{enumerate}
    Since \( \ad\colon \lG\to \GL(\lG)\) is an homomorphism (lemma \ref{LemadhomomadXadYadXY}) we have
    \begin{equation}
        \big[ \ad(X_{\alpha}),\ad(y) \big]=\ad(t_{\alpha})
    \end{equation}
    and then \( \tr\big( \ad(t_{\alpha}) \big)=0\) because the trace of a commutator is zero\footnote{From the cyclic invariance of the trace.}. Since \( V\) is an invariant subspace, the trace of \( \ad(t_{\alpha})\) restricted to \( V\) is also vanishing. Let us compute that trace on the basis \( \{ X_{\alpha},t_{\alpha},X^i_{m\alpha} \}_{m<0}\) where \( i\) takes as many values as the dimension of \( \lG_{m\alpha}\).

    We have \( \ad(t_{\alpha})X_{-\alpha}=-\alpha(t_{\alpha})X_{-\alpha}\), \( \ad(t_{\alpha})t_{\alpha}=0\) and \( \ad(t_{\alpha})X^i_{m\alpha}=m\alpha(t_{\alpha})X_{m\alpha}\), thus the trace is
    \begin{equation}    \label{Eqzequalmulsumnotinfty}
        0=\alpha(t_{\alpha})\Big( -1+\sum_{m=1}^{\infty}m\dim\lG_{m_{\alpha}} \Big).
    \end{equation}
    Notice that the sum is in fact finite since the dimension of \( \lG\) is finite. We know that \( \alpha(t_{\alpha})=B(t_{\alpha},t_{\alpha})\neq 0\), so that equation \eqref{Eqzequalmulsumnotinfty} is only possible with \( \dim\lG_{\alpha}=1\) and \( \dim\lG_{m\alpha}=0\) for \( m\neq 1\).
\end{proof}

A very similar proof can be found in \cite{Cornwell}, page 827. 


\begin{corollary}       \label{CorCoolWrlGbalpha}
    In the case of semisimple complex Lie algebra,
    
    \begin{enumerate}
        \item
            the root spaces are given by
            \begin{equation}        \label{EqExpWeightSemiSimple}
                \lG_{\alpha}=\{ X\in\lG\tq\forall h\in\lH, [h,X]=\alpha(h)X \};
            \end{equation}
        \item
            for every $ x_{\alpha}\in\lG_{\alpha}$, and for every $h\in\lH$, we have 
            \begin{equation}
                [h, x_{\alpha}]=\alpha(h) x_{\alpha}.
            \end{equation}
    \end{enumerate}
\end{corollary}

\begin{proof}
    Let \( X\in\lG_{\alpha}\), we have
    \begin{equation}
        \big( \ad(h)-\alpha(h) \big)^nX=0,
    \end{equation}
    so
    \begin{equation}    \label{EqadhalphahnnmuvX}
        \big( \ad(h)-\alpha(h) \big) \underbrace{\big( \ad(h)-\alpha(h) \big)^{n-1} X}_v=0.
    \end{equation}
    In particular the vector \( v= \big( \ad(h)-\alpha(h) \big)^{n-1} X\) belongs to \( \lG_{\alpha}\). Since the latter space has dimension one, the vector \( v\) is a multiple of \( X\) and consequently equation \eqref{EqadhalphahnnmuvX} shows that
    \begin{equation}
        \big( \ad(h)-\alpha(h) \big)v=\big( \ad(h)-\alpha(h) \big)X=0.
    \end{equation}

    The second point is only an other way to write the same.
\end{proof}

\begin{lemma}       \label{LemHzesialphaHz}
    If \( H\) is an element of \( \lH\) with \( \alpha(H)=0\) for every root, then \( H=0\)
\end{lemma}

\begin{proof}
    Consider the decompositions (not unique) \( H=\sum_{\alpha\in\Phi}a_{\alpha} t_{\alpha}\) and \( H'=\sum_{\beta\in\phi}a'_{\beta}t_{\beta}\). Then 
    \begin{subequations}
        \begin{align}
            B(H,H')&=\sum_{\alpha,\beta}a_{\alpha}a_{\beta}'B(t_{\alpha},t_{\beta})\\
            &=\sum_{\alpha,\beta}a'_{\beta}\beta(a_{\alpha},t_{\alpha})\\
            &=\sum_{\beta}a'_{\beta}\beta(H)\\
            &=0.
        \end{align}
    \end{subequations}
    Such an element is thus Killing-orthogonal to the whole space \( \lH\) but we already know the \( \lH\) is orthogonal to each space \( \lG_{\alpha}\) (\( \alpha\neq 0\)). By non degeneracy of the Killing form we must have \( H=0\).
\end{proof}

\begin{proposition}
    The set of roots of a complex semisimple Lie algebra spans the dual space \( \lH^*\).
\end{proposition}

\begin{proof}
    Consider a basis \( \{ H_i \}\) of \( \lH\) with \( \{ H_0,\ldots,H_m \}=\Span(\Phi)\) and \( \{ H_{m+1},\ldots,H_r \}\) be outside of \( \Span\Phi\). A root reads \( \alpha=\sum_{k=0}^ma_kH_k^*\).
    Thus \( \alpha(H_{m+1})=0\), which implies that \( H_{m+1}=0\) by lemma \ref{LemHzesialphaHz}.
\end{proof}

\begin{corollary}
    A Cartan algebra \( \lH\) of a complex semisimple Lie algebra is abelian.
\end{corollary}

\begin{proof}
    Let \( H',H''\in\lH\) and consider \( H=[H',H'']\), a root \( \alpha\) and \( X_{\alpha}\in\lG_{\alpha}\). On the one hand we have
    \begin{equation}
        \big[ [H',H''],X_{\alpha} \big]=-\alpha(H')[X_{\alpha},H']+\alpha(H')[X_{\alpha},H'']=0
    \end{equation}
    and on the other hand we have \( \big[ [H',H''],X_{\alpha} \big]=[H,X_{\alpha}]=\alpha(H)X_{\alpha}\). We deduce that \( \alpha(H)=0\) for every root and then that \( H=0\) by lemma \ref{LemHzesialphaHz}.
\end{proof}

We denote by \( \Phi\) the set of roots. These are the elements \( \lambda\in\lH^*\) such that \( \lG_{\lambda}\) is non trivial. We suppose to have chosen a positivity notion on \( \lH^*\), so that we can speak of \( \Phi^+\), the set of \defe{positive roots}{positive root}.

A positive root is \defe{simple}{simple!root} is it cannot be written as the sum of two positive roots.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Generators}
%---------------------------------------------------------------------------------------------------------------------------

We are going to build the Chevalley basis of the complex semisimple Lie algebra \( \lG\). That will essentially be a choice of a basis vector in each of the root spaces. We are following the notations summarized in point \ref{SubsecNotationRootsDel}.


Now, for each root $\alpha$, we pick $e_{\alpha}\in\lG_{\alpha}$. We will see that, up to renormalization, we can set the in nice commutation relations.

\begin{lemma}       \label{LemBalpahbetaef}
    If \( \alpha\) and \( \beta\) are roots such that \( \alpha+\beta\neq 0\), then 
    \begin{equation}
        B(e_{\alpha},e_{\beta})=0.
    \end{equation}
    If \( f_{\alpha}\in\lG_{-\alpha}\) we also have \( B(e_{\alpha},f_{\alpha})\neq 0\).
\end{lemma}

\begin{proof}
    By definition \( B(e_{\alpha},e_{\beta})=\tr\big( \ad(e_{\alpha})\circ\ad(e_{\beta}) \big)\). If we apply \( \ad(e_{\alpha})\circ\ad(e_{\beta})\) to an element of \( e_{\gamma}\) (including \( \lG_0=\lH\)), we get an element of \( \lG_{\alpha+\beta+\gamma}\). Thus the trace defining the Killing form is zero and \( B(e_{\alpha},e_{\beta})=0\) when \( \alpha+\beta=0\). 

    Since the Killing form is nondegenerate, we conclude that \( B(e_{\alpha},e_{-\alpha})\neq 0 \).
\end{proof}

\begin{corollary}       \label{CorrExistInverseRoot}
    Let \( \lG\) be a semisimple complex Lie algebra and \( \lH\) be a Cartan subalgebra of \( \lG\). Let \( \alpha\) be a root of \( \lG\) and \( \lH_{\alpha}=[\lG_{\alpha},\lG_{-\alpha}]\). There exist an unique \( H_{\alpha}\in\lH_{\alpha}\) such that \( \alpha(H_{\alpha})=2\).
\end{corollary}

\begin{proof}
    We have \( [e_{\alpha},f_{\alpha}]=B(e_{\alpha},f_{\alpha})t_{\alpha}\) and the lemma \ref{LemBalpahbetaef} shows that the Killing form is non zero. Multiplying by a suitable number provides the result.
\end{proof}
The element \( H_{\alpha}\in\lH\) defined in this lemma is the \defe{inverse root}{inverse!root} of \( \alpha\).

\begin{lemma}       \label{Lemalphaakbetaknimport}
    Let \( \{ \beta_1,\ldots,\beta_l \}\) be a choice of elements in \( \lH^*\) such that the set \( \{ t_{\beta_1},\ldots,t_{\beta_l} \}\) is a basis of \( \lH\). Thus the roots can be decomposed as
    \begin{equation}
        \alpha=\sum_{k=1}^la_k\beta_k
    \end{equation}
    with \( a_k\in\eQ\).
\end{lemma}

\begin{proof}

    Let \( \alpha=\sum_{k=1}^la_k\beta_k\). We know that the vectors \( t_{\beta_i}\) form a basis of \( \lH\), so we have the decomposition \( t_{\alpha}=\sum_ka_kt_{\beta_k}\). Indeed
    \begin{equation}
        B\big( h,\sum_k a_kt_{\beta_k} \big)=\sum_ka_k B(h,t_{\beta_k})=\sum_ka_k\beta_k(h)=\alpha(h).
    \end{equation}
    For each \( k=1,2,\ldots,l\) we have
    \begin{equation}
        (\alpha_k, \alpha) =\sum_{j=1}^la_k(\alpha_k, \alpha_j).
    \end{equation}
    This is a system of linear equations for the \( l\) variables \( a_k\). Since the coefficients \( (\alpha_k,\alpha)\) and \( (\alpha_k,\alpha_j)\) are rational by proposition \ref{PropScalrooTsQ}, the solutions are rational too.
\end{proof}

\begin{remark}
    The lemma \ref{Lemalphaakbetaknimport} deals with a quite general basis of \( \lH\). We will see in the proposition \ref{ThoposrootnjajnZ} that in the case of the basis of simple roots, the coefficients \( a_k\) are integers, either all positive or all negative.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Subalgebra \texorpdfstring{$ \gsl(2)_i$}{SL2R} }
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecCopiedeSLdansGi}

For each nonzero root \( \alpha\in\lH^*\), we choose \( e_{\alpha}\in\lG_{\alpha}\) and \( f_{\alpha}\in\lG_{-\alpha}\) in such a way to have
\begin{equation}
    B(e_{\alpha},f_{\alpha})=\frac{ 2 }{ B(t_{\alpha},t_{\alpha}) },
\end{equation}
and then we pose
\begin{equation}
    h_{\alpha}=\frac{ 2 }{ B(t_{\alpha},t_{\alpha}) }t_{\alpha}.
\end{equation}
Notice that these choices are possible because the Killing form is non degenerated on \( \lH\). 

% This proposition about gsl(2,R) was at position 198631779.
\begin{proposition}[\cite{SternLieAlgebra}] \label{PropWEzZYzC}
    For each root, the set $\{ e_{\alpha},f_{\alpha},h_{\alpha} \}$ generates an algebra isomorphic to $\gsl(2,\eR)$, i.e. they satisfy
    \begin{subequations}
        \begin{align}
            [h_{\alpha},e_{\alpha}]&=2e_{\alpha}\\
            [h_{\alpha},f_{\alpha}]&=-2f_{\alpha}\\
            [e_{\alpha},f_{\alpha}]&=h_{\alpha}\\
        \end{align}
    \end{subequations}
\end{proposition}

\begin{proof}
    Since \( \alpha(t_{\alpha})=B(t_{\alpha},t_{\alpha})\) we have \( \alpha(h_{\alpha})=2\). Now the computations are quite direct. The first is
    \begin{equation}
        [h_{\alpha},e_{\alpha}]=\alpha(h_{\alpha})e_{\alpha}=2e_{\alpha}.
    \end{equation}
    For the second,
    \begin{equation}
        [h_{\alpha},f_{\alpha}]=-\alpha(h_{\alpha})f_{\alpha}=-2f_{\alpha}.
    \end{equation}
    For the third, we know that \( [e_{\alpha},f_{\alpha}]\in\lH\); thus \( B\big( X,[e_{\alpha},f_{\alpha}] \big)=0\) whenever \( X\in\lG_{\lambda}\) with \( \lambda\neq 0\). Let \( h\in\lH\). Using the invariance of the Killing form,
    \begin{equation}
        B\big( h,[e_{\alpha},f_{\alpha}] \big)=B\big( [h,e_{\alpha}],f_{\alpha} \big)=\alpha(h)B(e_{\alpha},f_{\alpha})=B(t_{\alpha},t_{\alpha})B(e_{\alpha},f_{\alpha})=B\big( B(e_{\alpha},f_{\alpha})t_{\alpha},h \big).
    \end{equation}
    Thus
    \begin{equation}
        [e_{\alpha},f_{\alpha}]=B(e_{\alpha},f_{\alpha})f_{\alpha}=h_{\alpha}.
    \end{equation}
\end{proof}
Remark that we used the non degeneracy of the Killing form in a crucial way. The copy of \( \gsl(2,\eR)\) formed by \( \{ e_{\alpha},f_{\alpha},h_{\alpha} \}\) is denoted by $\gsl(2,\eR)_{\alpha}$. 

\begin{proposition}
    In the universal enveloping algebra,
    \begin{equation}        \label{Eqhjfikplusun}
        [h_j,f_i^{k+1}]=-(k+1)\alpha_i(h_j)f_i^{k+1}
    \end{equation}
    as generalisation of the previous one.
\end{proposition}

\begin{proof}
    We use an induction over $k$. Since $\ad(h_j)$ is a derivation in $\mU(\lG)$, the induction hypothesis and the definition relation $[h,f_i]=-\alpha_i(h)f_i$ with $h=h_i$, we have
    \begin{equation}
        \begin{split}
            \ad(h_j)f^{k+1}_i   &=\big( \ad(h_j)f_i^k \big)f_i+f_i^k\ad(h_j)f_i.\\
                        &=-k\alpha+i(h_j)f_i^kf_i-\alpha_i(h_j)f^{k+1}_i\\
                        &=-(k+1)\alpha_i(h_j)f_i^{k+1}.
        \end{split}
    \end{equation}
\end{proof}

Now the Lie algebra \( \lG\) can be seen as a \( \gsl(2,\eR)\)-module. As an example, for each choice of \( \beta\in\Phi\), the algebra \( \gsl(2)_{\alpha}\) acts on the vector space
\begin{equation}
    V=\bigoplus_{k\in\eZ}\lG_{\beta+k\alpha}.
\end{equation}
The vector space \( \lG\) carries thus several representations of \( \gsl(2)\); this fact will be used in a crucial way during the proof of proposition \ref{Proppqasbabaa}.
