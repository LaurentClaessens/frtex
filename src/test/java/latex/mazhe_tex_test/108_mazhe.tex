% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{Explicit choices in \texorpdfstring{$\so(2,l-1)$}{so2l-1}}
%\label{app_calc}
%+++++++++++++++++++++++++++++++++++++++++++

In the case of $AdS_4$ the matrices are $5\times 5$. We will write them down, but the general form are entirely similar. Our choice of Iwasawa decomposition is
\begin{subequations}
\begin{align}
\sN&=\{W_i,V_j,M,L\}\\
\sA&=\{ J_1, J_2\}.
\end{align}
\end{subequations}


The basis of $\sodn$ in which we want to decompose all our elements is the root space one:
\begin{equation}
\sG=\Span\{J_1,q_1,X,Y,V,W,M,N,F,L\}
\end{equation}
note in particular that $\sG_{(0,0)}=\Span\{J_1,q_1\}$ and $W,J_1\in\sH$.
\begin{equation}
\frac{1}{2}(W-Y)=
\begin{pmatrix}
&0\\
0&0&0&0&1\\
&0\\
&0\\
&1
\end{pmatrix},
\qquad
\frac{1}{2}(V+X)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&1\\
&&&&0\\
0&0&-1&0&0
\end{pmatrix},
\end{equation}



\begin{equation}
\frac{1}{2}(W+Y)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&0\\
&&&&1\\
0&0&0&-1&0
\end{pmatrix},
\qquad
q_3=\frac{1}{2}(V-X)=
\begin{pmatrix}
0&0&0&0&1\\
0\\
0\\
0\\
1
\end{pmatrix}.
\end{equation}


\subsection{Decompositions and commutators for \texorpdfstring{$\sQ$}{Q}}
%-------------------------------------------------------------------------

First, the root space decomposition of the basis $\{q_i\}$ of $\sQ$:
\begin{subequations}
\begin{align} 
	q_0	&=\frac{1}{ 4 }(M+N+L+F)				&	q_2	&=\frac{1}{ 4 }(N+F-M-L)\\
		&=\frac{1}{ 4 }(X_{11}+X_{1,-1}+X_{-1,1}+X_{-1,-1})	&		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})\\
	q_1	&=q_1=J_2						&	q_3	&=\frac{1}{2}(V-X)\\
		&							&		&=\frac{ 1 }{2}(X_{01}-X_{0,-1}).
\end{align}
\end{subequations}
The commutators:
\begin{subequations}
\begin{align} 
[q_0,q_1]&=\us{4}(L+F-M-N) &[q_1,q_2]&=\us{4}(L+N-F-M)\\\
&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})	&&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,1}-X_{-1,1}-X_{11})\\
[q_0,q_2]&=-J_1            &[q_1,q_3]&=\frac{1}{2}(V+X)\\
[q_0,q_3]&=\frac{1}{2}(Y-W)      &[q_2,q_3]&=\frac{1}{2}(W+Y)
\end{align}
\end{subequations}

\subsection{Commutators between root spaces and \texorpdfstring{$\sQ$}{Q}}

\begin{subequations}
\begin{align}
[q_0,J_1]	&=\frac{1}{4}(N+F-M-L)					&[q_1,J_1]&=0	&[q_2,J_1]&=q_0\\
		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})					\\
		&=q_2\\
[q_0,q_1]	&=\frac{1}{4}(L+F-M-N)&       				&    &[q_2,q_1]&=\us{4}(F+M-L-N)\\
		&=\frac{ 1 }{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})\\
[q_0,X]  &=\frac{1}{2}(W-Y)          &[q_1,X]  &=-X &[q_2,X]&=-\frac{1}{2}(W+Y)\\
[q_0,Y]  &=\frac{1}{2}(X-V)          &[q_1,Y]  &=0  &[q_2,Y]&=\frac{1}{2}(V-X)\\
[q_0,V]  &=\frac{1}{2}(Y-W)          &[q_1,V]  &=V  &[q_2,V]&=\frac{1}{2}(W+Y)\\
[q_0,W]  &=\frac{1}{2}(V-X)          &[q_1,W]  &=0  &[q_2,W]&=\frac{1}{2}(V+X)\\
[q_0,M]  &=q_1+J_1             &[q_1,M]  &=M  &[q_2,M]&=q_1+J_1\\
[q_0,N]  &=q_1-J_1             &[q_1,N]  &=N  &[q_2,N]&=-q_1+J_1\\
[q_0,L]  &=-q_1+J_1            &[q_1,L]  &=-L &[q_2,L]&=-q_1+J_1\\
[q_0,F]  &=-q_1-J_1            &[q_1,F]  &=-F &[q_2,F]&=q_1+J_1
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
 [q_3,J_1]&=0           &[q_3,M]&=W   &[q_3,V]&=-q_1\\
 [q_3,q_1]&=-\frac{1}{2}(V+X) &[q_3,N]&=-Y  &[q_3,W]&=\frac{1}{2}(M+L)\\
          &             &[q_3,L]&=W   &[q_3,X]&=-q_1\\
          &             &[q_3,F]&=-Y  &[q_3,Y]&=-\frac{1}{2}(N+F)
\end{align}
\end{subequations}

\subsection{Commutators in the root spaces}

\begin{subequations}
\begin{align}
[J_1,q_1]&=0\\
[J_1,X]&=0&[q_1,X]&=-X\\
[J_1,Y]&=-Y&[q_1,Y]&=0&[X,Y]&=F\\
[J_1,V]&=0&[q_1,V]&=V&[X,V]&=2q_1\\
[J_1,W]&=W&[q_1,W]&=0&[X,W]&=-L\\
[J_1,M]&=M&[q_1,M]&=M&[X,M]&=-2W\\
[J_1,N]&=-N&[q_1,N]&=N&[X,N]&=2Y\\
[J_1,F]&=-F&[q_1,F]&=-F&[X,F]&=0\\
[J_1,L]&=L&[q_1,L]&=-L&[X,L]&=0
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
[V,W]&=M\\
[V,M]&=0&[W,M]&=0\\
[V,N]&=0&[W,N]&=-2V&[M,N]&=0\\
[V,F]&=-2Y&[W,F]&=2X&[M,F]&=-4q_1-4J_1\\
[V,L]&=2W&[W,L]&=0&[M,L]&=0
\end{align}
\end{subequations}


\begin{subequations}
\begin{align}
[N,F]&=0\\
[N,L]&=-4q_1+4J_1&[F,L]&=0
\end{align}
\end{subequations}

\subsection{Killing form}
%++++++++++++++++++++
The adopted definition is $B(x,y)=\tr(\ad x\circ\ad y)$ with no one half or such coefficient.
\begin{equation}
\begin{aligned}
B(J_1,q_1)&=0	&B(V,X)&=-12\\
B(J_1,J_1)&=6	&B(N,L)&=-24\\
B(W,Y)&=-12	&B(M,F)&=-24
\end{aligned}
\end{equation}
Some easy computations show that for $g\in \SO(2)$,
\[
\begin{split}
dL_gq_0&=
\begin{pmatrix}
-\sin u&\cos u\\
-\cos u&\sin u
\end{pmatrix},
\quad
dL_g q_1=
\begin{pmatrix}
0&0&\cos u\\
0&&-\sin u\\
1
\end{pmatrix}\\
dL_g H_1&=
\begin{pmatrix}
0&0&\sin u\\
0&&\cos u\\
0&1
\end{pmatrix}\\
dR_g J_1&=
\begin{pmatrix}
0\\
0&0&0&1\\
0\\
-\sin u&\cos u
\end{pmatrix},
\quad
dR_g J_2=
\begin{pmatrix}
0&0&1\\
0\\
\cos u&\sin u
\end{pmatrix}
\end{split}
\]
So
\begin{subequations}
\begin{align}
dR_g J_1&=-\sin u\, dL_g q_2+\cos u\, dL_g H_2\\
dR_g J_2&=\sin u\, dL_g H_1+\cos u\, dL_g q_1.
\end{align}
\end{subequations}
and
\begin{subequations}
\begin{align}
  B_{[g]}(J_1^*,J_1^*)&=6\sin^2 u\\
B_{[g]}(J_2^*,J_2^*)&=6\cos^2 u.
\end{align}
\end{subequations}


\section{Iwasawa decomposition for \texorpdfstring{$\gsl(2,\eC)$}{sl2C}}		\label{SecIwasldeuxC}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{Iwasawa!decomposition!of $\SL(2,\eC)$}

Matrices of $\gsl(2,\eC)$ are acting on $\eC^2$ as 
\[ 
\begin{split}
  \begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}&
\begin{pmatrix}
a+bi\\c+di
\end{pmatrix}\\
&=
\begin{pmatrix}
(\alpha_1a-\alpha_2b+\beta_1c-\beta_2d)+i(\alpha_2a+\alpha_1b+\beta_2c+\beta_1d)\\
(\gamma_1a-\gamma_2b-\alpha_1c+\alpha_2d)+i(\gamma_2a+\gamma_1b-\alpha_2c-\alpha_1d)
\end{pmatrix}
\end{split}
\]
if $\alpha=\alpha_1+i\alpha_2$.  Our aim is to embed $\SL(2,\eC)$ in $\SP(2,\eR)$ (see sections \ref{SecSympleGp} and \ref{SecDirADs}), so that we want a four dimensional realization of $\gsl(2,\eC)$. It is easy to rewrite the previous action under the form of $\begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}$ acting of the vertical four component vector $(a,b,c,d)$. The result is that a general matrix of $\gsl(2,\eC)$ reads
\begin{equation}		\label{EqGenslMatr}
\gsl(2,\eC)\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&-\alpha_2\\
\alpha_2&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
\gamma_1&-\gamma_2\\
\gamma_2&\gamma_1
\end{array}&
\boxed{
\begin{array}{cc}
-\alpha_1&\alpha_2\\
-\alpha_2&-\alpha_1
\end{array}
}
\end{pmatrix}.
\end{equation}
The boxes are drawn for visual convenience.  Using the Cartan involution $\theta(X)=-X^t$, we find the following Cartan decomposition:
\begin{equation}
\begin{split}
\iK_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
0&-\alpha_2\\
\alpha_2&0
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix},\\
\iP_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&0\\
0&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix}.
\end{split}
\end{equation}
We have $\dim\iP_{\gsl(2,\eC)}=3$ and $\dim\iP_{\gsl(2,\eC)}=3$. A maximal abelian subalgebra of $\iP_{\gsl(2,\eC)}$ is the one dimensional algebra generated by
\[ 
  A_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix}.
\]
The corresponding root spaces are
\begin{itemize}
\item $\gsl(2,\eC)_0$:
\[ 
  I_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix},\quad
I_2=
\begin{pmatrix}
0&-1\\
1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsl(2,\eC)_2$:
\[ 
  D_1=\begin{pmatrix}
&&1&0\\
&&0&1\\
0&0\\
0&0
\end{pmatrix},\quad
D_2=
\begin{pmatrix}
&&0&-1\\
&&1&0\\
0&0&\\
0&0&
\end{pmatrix}
\]
\item $\gsl(2,\eC)_{-2}$
\[ 
  C_1=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&0\\
0&1
\end{pmatrix},\quad
C_2=\begin{pmatrix}
&&0&0\\
&&0&0\\
0&-1\\
1&0
\end{pmatrix}.
\]
\end{itemize}
It is natural to choose $\gsl(2,\eC)_2$ as positive root space system. In this case, $\iN_{\gsl(2,\eC)}=\{ D_1,D_2 \}$, $\iA_{\gsl(2,\eC)}=\{ I_1 \}$ and the table of $\iA\oplus\iN$ is
\begin{align}
[I_1,D_1]&=2D_1&		[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&		
\end{align}

    The full table is
\begin{align}
[I_1,D_1]&=2D_1&	[I_2,D_1]&=2D_2&	[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&	[I_2,D_2]&=-2D_1&	[D_1,C_1]&=I_1\\
[I_1,C_1]&=-2C_1&	[I_2,C_1]&=-2C_2&	[D_1,C_2]&=I_2\\
[I_1,C_2]&=-2C_2&	[I_2,C_2]&=2C_1&	[D_2,C_1]&=I_2\\
	&	&		&     &		[D_2,C_2]&=-I_1.
\end{align}
\section{Symplectic group}		\label{SecSympleGp}
%+++++++++++++++++++++++++

\subsection{Iwasawa decomposition}
%-----------------------------
\index{Iwasawa!decomposition!of $\SP(2,\eR)$}

A simple computation shows that $4\times 4$ matrices subject to $A^t\Omega+\Omega A=0$ are given by
\[ 
  \begin{pmatrix}
A&B\\
C&-A^t
\end{pmatrix}
\]
where $A$ is any $2\times 2$ matrix while $B$ and $C$ are symmetric matrices. Looking at general form \eqref{EqGenslMatr}, we see that the operation to invert the two last column and then to invert the two last lines provides a homomorphism $\phi\colon \gsl(2,\eC)\to \gsp(2,\eR)$. The aim is now to build an Iwasawa decomposition of $\gsp(2,\eR)$ which ``contains'' the one of $\gsl(2,\eC)$.

Using the Cartan involution $\theta(X)=-X^t$, we find the Cartan decomposition
\begin{align}
\iK_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
A&S\\-S&A
\end{pmatrix},
&\iP_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
S&S'\\S'&-S
\end{pmatrix}
\end{align}
where $S$ and $S'$ are any symmetric matrices while $A$ is a skew-symmetric one. We have $\dim\iK_{\gsp(2,\eR)}=4$ and $\dim\iP_{\gsp(2,\eR)}=6$. It turns out that $\phi(\iK_{\gsl(2,\eC)})\subset\iK_{\gsp(2,\eR)}$ and $\phi(\iP_{\gsl(2,\eC)})\subset \iP_{\gsp(2,\eR)}$. A maximal abelian subalgebra of $\iP_{\gsp(2,\eR)}$ is spanned by the matrices $A'_1$ and $A'_2$ listed below and the corresponding root spaces are:
\begin{itemize}
\item $\gsp(2,\eR)_{(0,0)}$:
\[ 
  A'_1=
\begin{pmatrix}
1&0\\
0&1\\
&&-1&0\\
&&0&-1
\end{pmatrix},
\quad
A'_2=
\begin{pmatrix}
0&1\\
1&0\\
&&0&-1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,2)}$:
\[ 
 X'= \begin{pmatrix}
1&-1&\\
1&-1&\\
&&-1&-1\\
&&1&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,-2)}$:
\[ 
 V'= \begin{pmatrix}
1&1\\
-1&-1\\
&&-1&1\\
&&-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,0)}$:
\[ 
 W'= \begin{pmatrix}
&&1&0\\
&&0&-1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,2)}$:
\[ 
  L'=
\begin{pmatrix}
&&1&1\\
&&1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,-2)}$:
\[ 
  M'=
\begin{pmatrix}
&&1&-1\\
&&-1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,0)}$
\[ 
Y'=
\begin{pmatrix}
&&0&0\\&&0&0\\
1&0\\0&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,2)}$:
\[ 
  N'=\begin{pmatrix}
&&0&0\\&&0&0\\
1&-1\\
-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,-2)}$:
\[ 
  F'=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&1\\
1&1
\end{pmatrix}
\]
\end{itemize}
It is important to notice how do the root spaces of $\gsl(2,\eC)$ embed:
\begin{align}
\phi(I_1)&=A'_1	&\phi(I_2)&=\frac{ V'-X' }{ 2 }\\
\phi(D_1)&=\frac{ L'-M' }{2}	&\phi(D_2)&=-W'\\
\phi(C_1)&=\frac{ F'-N' }{2}	&\phi(C_2)&=Y'. 
\end{align}
So $\iN_{\gsp(2,\eR)}$ must at least contain the elements $L'$, $M'$ and $W'$. We complete the notion of positivity by $V'$. The Iwasawa algebra reads
\[ 
\begin{split}
\iA_{\gsp(2,\eR)}&=\{ B_1,B_2 \}\\
\iN_{\gsp(2,\eR)}&=\{ L',M',W',V' \}
\end{split}  
\]
with
\begin{align*}
[L',V']&=-4W'	&[W',V']&=-2M'\\
[B_1',L']&=2L'	&[B'_2,M']&=2M'\\
[B_1',W']&=W'	&[B_2',W']&=W'\\
[B_1',V']&=-V'	&[B_2',V']&=V'
\end{align*}
where $B'_1=\frac{ 1 }{2}(A'_1+A'_2)$ and $B_2=\frac{ 1 }{2}(A_1'-A_2')$. The generators of $\iK_{\gsp(2,\eR)}$ are
\begin{align*}
K'_t&=
\begin{pmatrix}
&&1&0\\
&&0&1\\
-1&0\\
0&-1
\end{pmatrix}
	&K'_1&=
\begin{pmatrix}
0&1\\-1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}\\
K'_2&=
\begin{pmatrix}
&&0&1\\&&1&0\\0&-1\\-1&0
\end{pmatrix}
	&K'_3&=
\begin{pmatrix}
&&1&0\\
&&0&-1\\
-1&0\\
0&1
\end{pmatrix}.
\end{align*}
Notice that $[K'_t,K'_i]=0$ for $i=1$, $2$, $3$.


\subsection{Isomorphism}		\label{SubSecIsosp}
%-----------------------

The following provides an isomorphism $\psi\colon \so(2,3)\to \gsp(2,\eR)$:
\begin{align*}
\psi(H_i)&=B'_i		&\psi(u)&=K'_t\\
\psi(W)&=W'		&\psi(R_1)&=\frac{ 1 }{2}K'_1\\
\psi(M)&=M'		&\psi(R_2)&=\frac{ 1 }{2}K'_2\\
\psi(L)&=L'		&\psi(R_3)&=\frac{ 1 }{2}K'_3\\
\psi(V)&=\frac{ 1 }{2}V'
\end{align*}
where the $R_i$'s are the generators of the $\so(3)$ part of $\sK_{\so(2,3)}$ satisfying the relations $[R_i,R_j]=\epsilon_{ijk}R_k$. It is now easy to check that the image of the embedding $\phi\colon \gsl(2,\eC) \to \gsp(2,\eR)$ is exactly $\so(1,3)$, so that
\begin{equation}
\psi^{-1}\circ\phi\colon \gsl(2,\eC)\to \sH
\end{equation}
is an isomorphism which realises $\sH$ as subalgebra of $\gsp(2,\eR)$. This circumstance will be useful in defining a spin structure on $AdS_4$.

One can prove that the kernel of the adjoint representation of $\SP(2,\eR)$ on its Lie algebra is $\pm\mtu$, in other words, $\Ad(a)=\id$ if and only if $a=\pm\mtu$. We define a bijective map $h\colon \SO(2,3)\to \SP(2,\eR)/\eZ_2$ by the requirement that
\begin{equation}		\label{Eqdefhspsl}
  \psi\big( \Ad(g)X \big)=\Ad\big( h(g) \big)\psi(X)
\end{equation}
for every $X\in\so(2,3)$. The following is true for all $\psi(X)$:
\[ 
\begin{split}
\Ad\big(h(gg'\big)) \psi(X)&=\psi\Big( \Ad(g)\big( \Ad(g')X \big) \Big)\\
			&=\Ad\big( h(g) \big)\psi\big( \Ad(g')X \big)\\
			&=\Ad\big( h(g)h(g') \big)\psi(X),
\end{split}
\]
 the map $h$ is therefore a homomorphism. If an element $a\in \SP(2,\eR)$ reads $a= e^{X_A} e^{X_N} e^{X_K}$ in the Iwasawa decomposition, the property $\Ad(a)\psi(X)=\psi\big( \Ad(g)X \big)$ holds for the element\label{PgSolhpsiSP} $g= e^{\psi^{-1}X_A} e^{\psi^{-1}X_N} e^{\psi^{-1}X_K}$ of $\SO(2,3)$. This shows that $h$ is surjective.

\subsection{Reductive structure on the symplectic group}		\label{SubSecRedspT}
%-------------------------------------------------------

A lot of structure of $\so(2,3)$, such as the reductive homogeneous space decomposition as $\sQ\oplus\sH$, can be immediately transported from $\so(2,3)$ to $\gsp(2,\eR)$. Indeed, let $\mT=\psi(\sQ)$ and $\mI=\phi\big( \gsl(2,\eC) \big)$. We have the direct sum decomposition 
\[ 
\gsp(2,\eR)=\mT\oplus\mI.
\]
 Let $X\in\mT\cap\mI$, then $\psi^{-1}X$ belongs to $\sQ\cap\sH$ which only contains $0$. The fact that $\psi$ is an isomorphism yields that $X=0$. Since $\psi$ preserves linear independence, a simple dimension counting shows that the sum actually spans the whole space.

Putting $g=h^{-1}(a)$ in the definition \eqref{Eqdefhspsl} of $h$, we find
\[ 
  \psi\left( \Ad\big( h^{-1}(a) \big)X \right)=\Ad(a)\psi(X).
\]
Considering a path $a(t)$ with $a(0)=e$, we differentiate this expression with respect to $t$ at $t=0$ we find
\[ 
  \ad(dh^{-1}\dot a)X=d\psi^{-1}\big( \ad(\dot a)\psi(X) \big)=\ad(d\psi^{-1}\dot a)(d\psi^{-1}\psi X),
\]
but $d\psi=\psi$ because $\psi$ is linear, hence $[dh^{-1}\dot a,X]=[\psi^{-1}\dot a,X]$ for all $X\in \so(2,3)$ and $\dot a\in \gsp(2,\eR)$. We deduce that $(dh^{-1})_e=\psi^{-1}$. We define 
\begin{align*}
	\theta_{\gsp}&=\id|_{\iK_{\gsp}}\oplus(-\id)|_{\iP_{\gsp}}\\
	\sigma_{\gsp}&=\id|_{\mT}\oplus(-\id)|_{\mI}.
\end{align*}
We can check that $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$ and $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$. Then it is clear that
\[ 
  [\sigma_{\gsp},\theta_{\gsp}]=0
\]
using the corresponding vanishing commutator in $\so(2,3)$. We denote $\mT_a=dL_a\mT$ and the fact that $dp= d\pi\circ dh^{-1}= d\pi\circ \psi^{-1}$ shows that $dp(\mT_a)$ is a basis of $T_{p(a)}(G/H)$. So we consider the basis $t_i=\psi(q_i)$ of $\mT$ and the corresponding left invariant vector fields $\tilde t_i(a)=dL_at_i$.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Heisenberg group and algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Let $V$ be a symplectic vector space with the symplectic form $\Omega$. The \hypertarget{HyperHeisenberg}{Heisenberg algebra} build on $V$ is the vector space
\begin{equation}
	\pH(V,\Omega)=V\oplus \eR E
\end{equation}
endowed with the bracket defined by
\begin{enumerate}

	\item
		$[\pH(V,\Omega),E]=0$,
	\item
		$[v,w]=\Omega(v,w)E$ for every $v,w\in V$.

\end{enumerate}
The first conditions makes $E$ central in $\pH$.

The Heisenberg group is, as set, the same as the algebra : $H=V\oplus\eR E$ with the product
\begin{equation}		\label{EqProduitHeisenbergGp}
	g_1\cdot g_2=g_1+g_2+\frac{ 1 }{2}[g_1,g_2]
\end{equation}
where the bracket is the one in the Lie algebra. Direct computations show that this product is associative, the neutral is $(0,0)$ and that the inverse is given by
\begin{equation}
	g^{-1}=-g.
\end{equation}
We are now going to prove that the Lie algebra of that group actually is $\pH(V,\Omega)$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exponential mapping}
%---------------------------------------------------------------------------------------------------------------------------

Let us build the exponential map between the Heisenberg algebra and its group. Let $(x,\tau)\in T_eH$ and consider $g(s)= e^{s(x,\tau)}=\big( v(s),h(s) \big)$.  This map is subject to the following three relations:
\begin{enumerate}

	\item
		$g(s)g(t)=g(s+t)$,
	\item
		$g(0)=0$,
	\item
		$g'(01)=(x,t)$.

\end{enumerate}
Taking the derivative of the first one with respect to $s$ and taking into account $v'(0)=x$ and $h'(0)=\tau$, we find
\begin{subequations}
	\begin{align}
		\Dsdd{g(s)+g(t)+\frac{ 1 }{2}\big[ g(s),g(t) \big]  }{s}{0}&=\Dsdd{ \big( v(s+t),h(s+t) \big) }{s}{0}\\
		\Dsdd{ v(s)+v(t),h(s)+h(t)+\frac{ 1 }{2}\Omega\big( v(s),v(t) \big) }{s}{0}	&=\big( v'(t),h'(t) \big)\\
		\Big( x,\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big) \Big)&=\big( v'(t),h'(t) \big)
	\end{align}
\end{subequations}
We deduce that $v'(t)=x$ and $h'(t)=\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big)$, so that $v(t)=tx$ and $h(t)=t\tau$. The exponential mapping is thus given by the identity:
\begin{equation}
	\exp(x,\tau)=(x,\tau).
\end{equation}

In order to prove that the law \eqref{EqProduitHeisenbergGp} accepts the Heisenberg algebra as Lie algebra, we need to compute the adjoint action.
\begin{equation}
	\begin{aligned}[]
		\Ad( e^{t(x,\tau)})(x',\tau')&=\Dsdd{ \AD( e^{t(x,\tau)}) e^{s(x',\tau')} }{s}{0}\\
		&=\Dsdd{ (tx,t\tau)(sx',s\tau')(-tx,-t\tau) }{s}{0}\\
		&=\Dsdd{ \big(tx+sx',t\tau+s\tau'+\frac{ ts }{2}\Omega(x,x')\big)(-tx,t\tau) }{s}{0}\\
		&=\big( x',\tau'+t\Omega(x,x') \big).
	\end{aligned}
\end{equation}
Now, the Lie algebra bracket is given by
\begin{equation}
	\begin{aligned}[]
		\big[ (x,\tau),(x',\tau') \big]&=\Dsdd{ \Ad( e^{t(x,\tau)})(x',\tau') }{t}{0}\\
			&=\big( 0,\Omega(x,x') \big)\\
			&=\Omega(x,x')E,
	\end{aligned}
\end{equation}
which is the bracket of $\pH(V,\Omega)$.

\section{The group \texorpdfstring{$SU(2)$}{SU2}}
%--------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Definition and properties}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The group $\SU(2)$ is made of unitary $2\times 2$ matrices with unit determinant: $\det U=1$ and $U^{\dag} U=\mtu$. Since it naturally acts on $\eC^2$, we will describe some ``matricial``\ notations for the main operations in $\SU(2)$ and $\eC^2$. An element of $\eC^2$ is written as a column matrix, while the adjoint is an horizontal one :
\[
   \xi=\begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix},\quad 
   \eta^{\dag}=\begin{pmatrix}
\oeta_1  &\oeta_2
\end{pmatrix}.
\]
This suggests us a scalar product\footnote{Be careful: it is \emph{not} $\scal{\xi}{\eta}=\xi^{\dag}\eta$.} :
\[
   \scal{\xi}{\eta}=\eta^{\dag}\xi= \begin{pmatrix}
                                     \oeta_1  &\oeta_2
                                   \end{pmatrix}
   \begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix}=\xi_1\oeta_1+\xi_2\oeta_2.
\]
This scalar product is hermitian and in particular, it satisfies $\scal{\xi}{\eta}=\overline{\scal{\eta}{\xi}}$. Elements of $\SU(2)$ preserves this product because 
\[
   \scal{A\xi}{A\eta}=(A\eta)^{\dag}(A\xi)=\eta^{\dag} A^{\dag} A\xi=\scal{\xi}{\eta}
\]
for all $A\in\SU(2)$. One can easily understand that $\SU(2)$ is a compact group. Indeed, the unitary property gives :
\[
UU^{\dag}=
\begin{pmatrix}
\alpha & \beta \\ 
\gamma & \delta
\end{pmatrix} 
\begin{pmatrix}
\oalpha & \ogamma \\ 
\obeta & \odelta
\end{pmatrix}
=
\begin{pmatrix}
\alpha\oalpha+\beta\obeta & x \\ 
x & \gamma\ogamma+\delta\odelta
\end{pmatrix}
\stackrel{!}{=}
\begin{pmatrix}
1  & 0 \\ 
0 & 1
\end{pmatrix}.
\]
If $\alpha=x+iy$ and $\beta=a+bi$, one immediately has  $x^2+y^2+a^2+b^2=1$ (the same is true for $\gamma$ and $\delta$), so that $\SU(2)$ is contained in a bounded subset of $\eR^8$, and it is clear that $\SU(2)$ is closed in $\eR^8$ because it is defined by equalities.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Haar measure on \texorpdfstring{$\SU(2)$}{SU2}}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The quaternion\index{quaternion} field $\eH$ can be embed in $\eM_2(\eC)$ as a genera element reads
\begin{equation}
	q=
\begin{pmatrix}
  \alpha	&	\beta	\\ 
  -\bar\beta	&	\bar\alpha	
\end{pmatrix}
\end{equation}
with $\alpha$, $\beta\in\eC$. Under that isomorphism, we have
\[ 
	| q |^2=| \alpha |^2+| \beta |^2=\det q.
\]
Thus we have the identification
\begin{equation}
	\SU(2)=\{ q\in\eH\tq | q |=1 \}.
\end{equation}
We can act on $\eH$ by $\SU(2)\times \SU(2)$ by
\begin{equation}
	(u,v)\cdot q=uqv^{-1}
\end{equation}
for every $(u,v)\in \SU(2)\times\SU(2)$ and $q\in\eH$. That action defines an homomorphism from $\SU(2)\times\SU(2)$ onto $O(4)$.

\begin{proposition}
The previously defined homomorphism
\[ 
	\phi\colon \SU(2)\times\SU(2)\to O(4).
\]
is surjective over $\SO(4)$ (which is the identity component of $O(4)$) and, moreover, the kernel is $\big\{  (e,e),(-e,-e) \big\}$.
\end{proposition}

\begin{proof}
The group $\SU(2)\times \SU(2)$ being connected, its image can only be included in $\SO(4)$. Let us first determine the kernel of $\phi$. If $(u,v)\in\ker\phi$, we have $uqv^{-1}=q$ for every $q\in\eH$. In particular, with $q=1$, we find $u=v$. Then the relation $uqu^{-1}=q$ means that $u$ belongs to the center of $\eH$, which is $\eR$. We conclude that $u=\pm 1$. That proves that $\ker\phi=\big\{  (e,e),(-e,-e) \big\}$.

The differential $(d\phi)_{(e,e)}$ is an homomorphism
\[ 
	d\phi\colon \gsu(2)\oplus\gsu(2)\to \so(4).
\]
Let $(S,T)\in\gsu(2)\oplus\gsu(2)$, we have
\[ 
	d\phi(S,T)q=\Dsdd{ \phi( e^{t(S,T)})q }{t}{0}=\Dsdd{ \phi( e^{tS}, e^{tT})q }{t}{0}=\Dsdd{  e^{tS}q e^{-tT} }{t}{0}=Sq-qT,
\]
on which one sees that $d\phi$ is injective. Moreover we have $\dim\big( \gsu(2)\oplus\gsu(2) \big)=6=\dim\so(4)$. An injective map between vector space of same dimension being an isomorphism, the image of $\phi$ contains a neighborhood of identity in $\SO(4)$. From connectedness of $\SO(4)$, that neighborhood generates the whole group (see proposition \ref{PropUssGpGenere}), so that $\phi$ is in fact surjective.
\end{proof}

Since the map $\phi\colon \SU(2)\times \SU(2)\to \SO(4)$ is a surjective homomorphism with a discrete kernel, we have an isomorphism at the algebra level:
\[ 
	\so(4)\simeq \gsu(2)\oplus\gsu(2).
\]

\subsection{Building some representations for \texorpdfstring{$\SU(2)$}{SU2}}
%/////////////////////////////////////////////////////////////////////////////////

Since $\SU(2)$ acts on $\eC^2$, we can build a representation of $\SU(2)$ on functions on $\eC^2$. We define $\dpt{T}{SU(2)}{\End\big(\Cinf(\eC^2)\big)}$ by 
\[
  (T(U)f)(\xi)=f(U^{-1}\xi),
\]
if $f\in\Cinf\big(\eC^2\big)$, $\xi\in\eC^2$ and $U\in SU(2)$. 

Let $V_j$ be the space of the homogeneous polynomials of degree $j$ on $\eC^2$; a basis of this space is given by the $\phi_{pq}$, $p+q=2j$ defined by
\begin{equation}
   \phi_{pq}(\xi)=\xi_1^p\xi_2^q
\end{equation}
($\xi=\xi_1+i\xi_2$). If $j$ is fixed, we will often write $\phi_m$ instead of $\phi_{pq}$. The signification is $p=j+m$, $q=j-m$, and $m$ takes its values in $-j,\ldots,j$. Note that $p-q=2m$. It is clear that if $A$ is any invertible $2\times 2$ matrix , and $f\in V_j$, then 
\[
   \rho(A)f:=f(A^{-1} \cdot)
\]
 is still an element of $V_j$. This representation $\rho$ is defined on the whole $\Cinf(\eC^2)$. We will descent it to $V_j$ later.
Now, we fix $j$ and a $m$ between $-j$ and $j$. 

Consider the diagonal matrix
\[   U_{-\theta}=\begin{pmatrix}
e^{-i\theta} & 0 \\ 
0 & e^{i\theta}
\end{pmatrix} \in\SU(2).
\]
One has
\begin{equation}
  \left(\rho(U_{-\theta})\phi_{pq}\right)(\xi)=\phi_{pq}
\begin{pmatrix}
   e^{i\theta}\xi_1 \\ 
    e^{-i\theta}\xi_2
\end{pmatrix}
                                               = e^{pi\theta} e^{-qi\theta}\xi_1^p\xi_2^q
					       =e^{2mi\theta}\phi_{pq}(\xi).
\end{equation}
First conclusion: the $\phi$'s are eigenvectors of $\rho(U_{-\theta})$ because
\[
   \rho(U_{-\theta})\phi_m=e^{2mi\theta}\phi_m.
\]
Second, the trace of $\rho(U_{-\theta})$ is
\begin{equation}
   \chi_j(\theta)=\sum_{m=-s}^{s}e^{2mi\theta}.
\end{equation}
By the way, the $\chi_j$ are the characters of the representation $\rho$.

From considerations about the Haar\quextproj{} invariant measure on $\SU(2)$, one knows that the good notion product between functions is :
\begin{equation}
(f_1,f_2)_{\SU(2)}=\frac{2}{\pi}\int_0^{\pi}f_1(\theta)\overline{ f_2(\theta) }\sin^2\theta\,d\theta,
\end{equation}
so that $(\chi_j,\chi_j)=1$. This and the fact that $\SU(2)$ is compact make the theorem of Peter-Weyl (cf. \cite{Sternberg}) applicable, thus the restrictions of $\rho$ to the $V_j$'s are irreducible and moreover, these provide \emph{all} the irreducible representations.

\subsection{Special case: \texorpdfstring{$j=\frac{1}{2}$}{j=1/2}}
%//////////////////////////////////////////////////////////////////////

Consider a matrix $A\in\SU(2)$ : 
\begin{equation}
A=\begin{pmatrix}
\oalpha & -\beta \\ 
\obeta & \alpha
\end{pmatrix},\qquad
A^{-1}=\begin{pmatrix}
\alpha & \beta \\ 
-\obeta & \oalpha
\end{pmatrix}.
\end{equation}
A basis of $V_{\frac{1}{2}}$ is given by $\phi_{10}$ and $\phi_{01}$. Let us see how $\rho(A)$ acts on. Since
\[
A^{-1}\begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix}=
\begin{pmatrix}
\alpha\xi_1+\beta\xi_2 \\ 
-\obeta\xi_1+\oalpha\xi_2
\end{pmatrix},
\]
we find
\begin{equation}
\begin{split}
  (\rho(A)\phi_{10})(\xi)&=\alpha\xi_1+\beta\xi_2=(\alpha\phi_{10}+\beta\phi_{01})(\xi)\\
  (\rho(A)\phi_{01})(\xi)&=-\obeta\xi_1+\oalpha\xi_2=(-\obeta\phi_{10}+\oalpha\phi_{01})(\xi).
\end{split}
\end{equation}
Thus in the basis $\{\phi_{10},\phi_{01}\}$, the matrix of $\rho(A)$ is given by
\begin{equation}
\rho(A)=\begin{pmatrix}
\alpha & -\obeta \\ 
\beta & \oalpha
\end{pmatrix}=\overline{A}.
\end{equation}

Up to here, we were looking at the representation $\rho$ of $\SU(2)$ on the whole set of functions on $\eC^2$, and more precisely, its restriction to $V_j$. We could define the representation $\rho_{\frac{1}{2}}$ as $\rho_{\frac{1}{2}}=\rho|_{V_{\frac{1}{2}}}$, but we will not do it. Our definition is
\begin{equation}
  \rho_{\frac{1}{2}}(A)=\rho(\overline{A})|_{V_{\frac{1}{2}}}.
\end{equation}
Note that 
\[
\begin{pmatrix}
0 & -1 \\ 
1 & 0
\end{pmatrix}
A 
\begin{pmatrix}
0 & 1 \\ 
-1 & 0
\end{pmatrix}=\overline{A},
\]
thus the representation $A\to\rho(\overline{A})|_{V_{\frac{1}{2}}}$ is equivalent to $A\to\rho(A)|_{V_{\frac{1}{2}}}$. This equivalence can also be seen because these two representations have the same characters\quextproj.

The basis $\phi_{pq}$ is orthogonal; we will build an orthonormal one: $e_m(\xi)$ is the vector whose coordinates are
\begin{equation}
e_m^j(\xi)=\frac{ \xi_1^{j+m}\xi_2^{j-m} }{\sqrt{ (j+m)!(j-m)! }}
\end{equation}
for $m=-j,-j+1,\ldots,j$. The metric to take in order to define $(e_m,e_n)$ is the unique one on $V_j$ which is $\SU(2)$-invariant.

The Newton's formula for the binomial yields :
\begin{equation}
\begin{split}
  \sum_{m=-s}^s e_m(\xi)\overline{e_m(\eta)}
        &=\sum\frac{ \xi_1^{j+m}\xi_2^{j-m}\oeta_1^{j+m}\oeta_2^{j-m} }{(j+m)!(j-m)!}\\
	&=\us{(2j)!}(\xi_1\oeta_1+\xi_2\oeta_2)^2\\
	&=\us{(2j)!}\scal{\xi}{\eta}^{2j}.
\end{split}
\end{equation}
But we know that $A\in\SU(2)$ preserves the scalar product: $\scal{A\xi}{A\eta}=\scal{\xi}{\eta}$. Therefore :
\begin{equation}\label{eq:produit_e_m}
\sum (\rho_j(A)e_m)(\xi)\overline{ (\rho_j(A)e_m)(\eta) }=\sum e_m(\xi)\overline{e_m(\eta)}.
\end{equation}
Now, instead of considering the matrices $\rho_j(A)$ on $V_j$ for the basis $\phi_m$, we looks at the ones with respect to the basis $e_m$ :
\begin{equation}
\rho_j(A)e_m=r(A)^k_me_k;
\end{equation}
in others words, we looks at the representation $A\to r(A)$. The equations \eqref{eq:produit_e_m} makes
\[
  \sum_{m=-j}^j\left(
                      r(A)^l_me_l(\xi)\overline{ r(A)^k_me_k(\eta)   }
		        -\delta^l_me_l(\xi)\delta^k_me_k(\eta)
                \right)=0.
\]
Since the functions
\begin{equation}
\begin{aligned}
 e_k\otimes\overline{e_l}\colon \eC^2\times\eC^2 &\to \eC \\ 
(\xi,\eta) &\mapsto  e_k(\xi)\overline{e_l(\eta)}
\end{aligned}
\end{equation}
 are linearly independent, one gets $\sum_m r(A)^k_l\overline{r(A)^l_m}=\delta^{kl}$, or
\begin{equation}
r(A)r(A)^*=\mtu,
\end{equation}
the conclusion is that in this basis, the matrices $\rho_j(A)$ are unitary.

\subsection{Clebsch-Gordan}
%//////////////////////////////////////////////////////////////////////

From the knowledge of the characters of $\rho_j$, one can decompose the product $\rho_s\otimes\rho_r$ into irreducible representations. For example,
\[
   V_{\frac{1}{2}}\otimes V_{\frac{1}{2}}=V_0\oplus V_1.  
\]
More generally,
\begin{equation}
  V_s\otimes V_r=V_{|r-s|} \oplus V_{|r-s|+1}\oplus\ldots\oplus V_{r+s}.
\end{equation}
For this reason, the representation $\rho_j$ is sometimes called the \defe{spin $j$}{spin!representation!of $\SU(2)$} representation of $\SU(2)$.

%---------------------------------------------------------------------------------------------------------------------------
					\section{Representations of \texorpdfstring{$\SL(2,\eR)$}{SL2R} and \texorpdfstring{$\SU(2)$}{SU2}}
%---------------------------------------------------------------------------------------------------------------------------

The representation $\pi_m$ of $\SL(2,\eC)$ restricts to $\SL(2,\eR)$.

\begin{lemma}
The representation $\pi_m$ of $\SL(2,\eR)$ is irreducible.
\end{lemma}

\begin{proof}
If $W$ is an invariant space under $\pi_m\big( \SL(2,\eR) \big)$, then is is invariant under the derived representation $\rho_m\big( \gsl(e,\eR) \big)$. The proof of proposition \ref{ProprhomirredsldeuxC} still holds here, so that $W=\mP_m$.
\end{proof}

\begin{theorem}
Let $\pi$ be an irreducible representation of $G=\SL(2,\eR)$ or $\SU(2)$ in a complex finite dimensional vector space $V$. Then $\pi$ is equivalent to one of the $\pi_m$.
\end{theorem}

\begin{proof}
Let $\lG$ be the Lie algebra of $G$. One important property shared by $\SL(2,\eR)$ and $\SU(2)$ is that $G=\exp(\lG)$. It is clear that the representation $d\pi$ on $\gsl(2,\eR)$ extends $\eC$-linearly to a representation $\rho$ of $\gsl(2,\eC)$. Looking on the basis \eqref{EqGenssudeux}, one sees that in fact the same is true for $\gsu(2)$ which $\eC$-linearly extends to $\gsl(2,\eC)$.

Let us prove that $\rho$ is irreducible. Let $W\neq\{ 0 \}$ be a subspace of $V$ invariant under $\rho(\lG)$. Then $W$ is invariant under $ e^{\rho(X)}=\pi( e^{X})$ for every $X\in\lG$. Since $\exp(\lG)=G$, the space $W$ is in fact invariant under $\pi(G)$, and is therefore equal to $V$.

Since $\rho$ is irreducible, we have $\rho=\rho_m$ for a certain $m$. Thus there exists an intertwining operator $A\colon V\to \mP_m$ such that
\[ 
	A\rho(X)=\rho_m(X)A
\]
for every $X\in\lG$. By linearity, for every $N\in\eN$, we have $A\rho\big( \sum_{k=1}^n X^k/k! \big)=\rho_m\big( \sum_{k=1}^n X^k/k! \big)A$, and at the limit, we have
\begin{equation}
	A e^{\rho(X)}= e^{\rho_m(X)A}.
\end{equation}
From that we deduce that $A\pi( e^{X})=\pi_m( e^{X})A$ which means that
\[ 
	A\pi(g)=\pi_m(g)A.
\]
That shows that $A$ intertwines $\pi$ and $\pi_m$, so that $\pi$ is equivalent to $\pi_m$.
\end{proof}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Representations of \texorpdfstring{$\so(2,d-1)$}{so2d}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here we deal with the representations of \( \so(2,d-1)\). For singleton theory as field theory, see the section \ref{SecUKPhZVd} (which is almost empty).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Verma module}
%---------------------------------------------------------------------------------------------------------------------------

One can find text about these representations in \cite{Ferrata,Dolan_son,SingString}, while we will mainly follow the developments of \cite{SingletonCompposites,HowMassless,Teschner}. We are going to study representations of the algebra $\so(2,d-1)$ which fulfills the commutation relations described in lemma \ref{LemCommsopqAlg}:
\begin{equation}		\label{EqCommsodeuxdmoinsun}
	[M_{ab},M_{cd}]=-i\eta_{ac}M_{bd}+i\eta_{ad}M_{bc}+i\eta_{bc}M_{ad}-i\eta_{bd}M_{ac}.
\end{equation}
Notice that $M_{ab}=-M_{ba}$. As convention, the indices $a$, $b$,\ldots run over $\{ 0,0',1,2,\ldots d-1 \}$ while $r$, $s$, \ldots run over $\{ 1,2,\ldots,d-1 \}$. As on page \pageref{PgDefsGenre}, we choose the convention
\begin{equation}
	\eta =
	\begin{pmatrix}
		\mtu_{2\times 2}\\
		&-\mtu_{(d-1)\times (d-1)}
	\end{pmatrix}.
\end{equation}
Notice that this convention numerically holds for the matrix $\eta_{st}$ as well as for its inverse $\eta^{st}$.

The algebra separates into two parts: the compact and the non compact part. The maximal compact subalgebra is $\so(2)\oplus\so(d-1)$ which is generated by $E=M_{00'}$ and $J_{rs}=M_{rs}$. The non compact generators are $M_{0'r}$ and $M_{0r}$ that we rearrange into ladder operators
\begin{equation}
	L^{\pm}_r=M_{0r}\mp iM_{0'r}.
\end{equation}
Using commutation relations \eqref{EqCommsodeuxdmoinsun}, one computes the commutators in the new basis. For example
\[ 
	[E,L^{\pm}_r]=[M_{0'0},M_{0r}]\mp i[E_{0'0},M_{0'r}]=\pm M_{0r}-iM_{0'r}=\pm L^{\pm}_r.
\]
The table of $\so(2,d-1)$ in this basis is
\begin{subequations}		\label{SubEqsCommssodeuxd}
	\begin{align}
		[E,L^{\pm}_r]&=\pm L^{\pm}_r\\
		[J_{rs},L_t^{\pm}]&=-i(\delta_{rt}L_s^{\pm}-\delta_{st}L_r^{\pm})\\
		[L_r^{-},L_s^+]&=2(iJ_{rs}+\delta_{rs}E)\\
		[J_{rs},J_{tu}]&=-i\delta_{ac}M_{bd}+i\delta_{ad}M_{bc}+i\delta_{bc}M_{ad}-i\delta_{bd}M_{ac}.
	\end{align}
\end{subequations}
The unitary properties are $(M_{rs})^{\dag}=M_{rs}$, $E^{\dag}=E$ and $(L^{\pm}_r)^{\dag}=L_r^{\mp}$. From these commutators, we deduce the following rules that will be always used
\begin{subequations}
	\begin{align}
		L^-_rL^+_s&=L^+_sL^-_r+2(iJ_{rs}+\delta_{rs}E)\\
		J_{rs}L^+_t&=L^+_tJ_{rs}-i(\delta_{rt}L^+_s-\delta_{st}L^+_r)\\
		EL^+_r&=L^+_rE+L^+_r.
	\end{align}
\end{subequations}

The Cartan algebra of $\so(d)$ is given by the elements $A_p=M_{2p-1,2p}$ with $p=1,\ldots, r$ for $\so(2r)$ and $\so(2r+1)$. 

The unitary irreducible representations of $\so(2,n)$ have the form $\mD(e_0,\bar\jmath)$. It is given by a basis vector $| e_0,\bar\jmath \rangle$ on which $E$ and $J_{rs}$ act by their respective representations (of $\so(n)$ and $\so(2)$). The \defe{energy}{energy!in the representations of $\so(2,d-1)$} of the vector $\ket{e,\overline{ m }}$ is its eigenvalue for the operator $E$, namely $e$:
\begin{equation}
	E\ket{e,\overline{ m }}=e\ket{e,\overline{ m }}.
\end{equation}
Using the commutators \eqref{SubEqsCommssodeuxd}, we find $L_r^{\pm}=(E\pm 1)L_r^{\pm}$, so that
\begin{equation}
	E L^{\pm}_r\ket{e,\overline{ m }} =(e\pm 1)\ket{e,\overline{ m }}.
\end{equation}
We see that the ladder operator $L_r^+$ raises the value of the energy of one unit, while the operator $L_r^-$ lower the energy of one unit. The vector $\ket{e_0,\bar\jmath}$ is the \defe{vacuum vector}{vacuum!vector}, it has the lowest energy in the sense that $L^{-}_r| e_0,\jmath \rangle=0$. A \defe{scalar representation}{scalar!representation} is a representation with $\bar\jmath=0$. They are, logically, denoted by $\mD(e_0)$ and its vacuum is $| e_0 \rangle$ which satisfies
\begin{align}		\label{Eqaldefketezerovac}
	J_{rs}| e_0 \rangle & = 0	& (E-e_0)| e_0 \rangle&=0	&L_r^{-}| e_0 \rangle&=0.
\end{align}
Then one build the generalised Verma module
\begin{equation}	\label{EqmVVermaldots}
	\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}.
\end{equation}
Notice that the Verma module is not automatically irreducible. We will soon build irreducible representations by taking quotient of the Verma module by its singular module.

In order to compute the norm of $L_s^+L_r^+\ket{e_0}$, we compute $L^-_rL^-_sL^+_sL^+_r\ket{e_0} =4\big(E+E^2+\delta_{rs}E^2+(J_{rs})^2\big)\ket{e_0}$. In order to get that result, we moved all the $L^-$ on the right using the commutation relation, and we taken into account the simplifications induced by the definition relations \eqref{Eqaldefketezerovac}. Now, using the relation $J_{rs}\ket{e_0}=0$, we have
\begin{equation}
	4\big(E+E^2+\delta_{rs}E^2\big)\ket{e_0}.
\end{equation} We also have
\begin{equation}
	(E-e_0)L^+_sL^+_s\ket{e_0}=0.
\end{equation}

\begin{proposition}
The vectors $L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}$ and $L^+_{t_1}\ldots L^+_{t_l}\ket{e_0}$ are orthogonal if $k\neq l$.
\end{proposition}
That proposition says that different layers are orthogonal\quext{À justifier en analysant qui est exactement $\lH$ et les racines simples, mais ça me semble ok.}

\begin{proof}
We proceed by induction. We suppose that the result is proved for $k,l\geq n$, and we prove that 
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}=0.
\end{equation}
First, remark that, using the commutation relations and the fact that $J_{rs}\ket{e_0}=0$ and $E\ket{e_0}=e_0\ket{e_0}$, the vectors
\begin{subequations}		\label{SubEqsJELLket}
	\begin{align}
		J_{st}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}\\
		EL^+_{r_1}\ldots L^+_{r_k}\ket{e_0}
	\end{align}
\end{subequations}
are combinations of vectors of the form $L^+_{a_1}\ldots L^+_{a_k}\ket{e_0}$. Now, we have
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}= L^-_{t_1}\ldots L^-_{t_n}\big( L^+_{r_1}L^-_{t_{n+1}}+2i(J_{t_{n+1},r_1} + \delta_{t_{n+1},r_1}E ) \big)L^+_{r_2}\ldots L^+_{r_n}\ket{e_0}
\end{equation}
which decomposes in three terms. The first one is
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_n}L^+_{r_1}L^-_{t_{n+1}}L^+_{r_2}\ldots L^+_{r_n}\ket{e_0},
\end{equation}
and according to equations \eqref{SubEqsJELLket}, the two other terms reduce to zero. Continuing that way, the operator $L^-_{t_{n+1}}$ advance of one position at each step and finishes to kill himself on $\ket{e_0}$.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Singular module}
%---------------------------------------------------------------------------------------------------------------------------

Let us compute the norm of the general vector $L^+_{r_1}\ldots L^+_{r_k}\ket{s_0,s}$. We have
\begin{equation}
	\begin{aligned}[]
		L^-_{r_k}\ldots L^-_{r_1}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0,s}	
				&= L^-_{r_k}\ldots L^-_{r_2} (L^+_{r_1}L^-_{r_2}+2E) L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&= L^-_{r_k}\ldots L^-_{r_2}L^+_{r_1}L^-_{r_2}L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&\quad +2(e_0+k-1)L^-_{r_k}\ldots L^-_{r_2} L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}
	\end{aligned}
\end{equation}
Using again and again the commutation relations, we eliminate all the operators $L^+_r$ and we obtain a sum of terms of the form $(e_0+k-l)$. It will, obviously be positive for large enough $e_0$. Thus, unitarity of the representation is enforced for large values of $e_0$, and there exists a lower bound $E_0(s)$ such that negative norm states appears when $e_0<E_0(s)$. If $e_0=E_0(s)$, then these vectors have a vanishing norm.

Let us consider a limit representation: $e_0=E_0$; there are vectors of vanishing norm, but no vectors with negative norm. In that case, if $v\cdot v=0$, then $v\cdot w=0$ for every other vector $w$. Indeed, if $v\cdot w\neq 0$, we have
\begin{equation}
	(v-w)\cdot(v-w)=w\cdot w - v\cdot w-w\cdot v,
\end{equation}
which holds for every positive multiple $\lambda v$ and $\mu w$. Choosing a big $\lambda$ and a small $\mu$, the norm of $\lambda v-\mu w$ becomes negative. What we proved is
\begin{lemma}
	If the energy $e_0$ of a representation saturates the unitary condition, then a vector with vanishing norm is orthogonal to every other vectors. Moreover, the vectors with vanishing norm form an invariant subspace.
\end{lemma}
The second part is the fact that, if $A\in\lG$, and $\| \ket{\psi} \|=0$  then $\| A\ket{\psi} \|=0$, because it is the scalar product of $\ket{\psi}$ with the vector $A^{\dag}A\ket{\psi}$. The submodule made of vectors of zero norm is the \defe{singular submodule}{singular!submodule}, and is denoted by $\mS(e_0,s)$.

\begin{proposition}		\label{PropSinModRedSSIADesNuls}
A module is reducible if and only if it possesses a vector $\ket{v}$ (different from $\ket{e_0,s}$) such that $L^-_r\ket{v}=0$ for every $r$. Such a vector is said to be \defe{null}{null vector}.
\end{proposition}

\begin{proof}
Since the energy is bounded from bellow, applying several times the lowering operators $L^-_r$ on any vector ends up on zero. Thus, any submodule contains a vector $\ket{v}$ such that $L^-\ket{v}=0$ for every $r$. If that vector is not $\ket{e_0,s}$, then the submodule is a proper submodule.

If $\ket{v}\neq\ket{e_0,s}$, then it is of the form $L^+_{\bar r}\ket{e_0,s}$ and its norm is given by
\begin{equation}
	\| L^+_{\bar r}\ket{e_0,s} \|=\bra{e_0,s} L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0
\end{equation}
because $L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0$ by assumption.
\end{proof}
From the Verma module \eqref{EqmVVermaldots}, we thus extract the irreducible representation taking the quotient by the singular module:
\begin{equation}
	\mH(e_0) = \mV(e_0)/\mS(e_0).
\end{equation}

Most of time, we have only one extra vacuum, let $\ket{e'_0,s'}$, and in this case, the whole singular module is generated by vectors of the form
\begin{equation}
	L^+_{r_1}\ldots L^+_{r_k}\ket{e'_0,s'}.
\end{equation}
Let $\ket{e_0,s}$ be the vacuum with $s=(s_1,s_2,0,\ldots,0)$, corresponding to the Young diagram
\begin{equation}
   \input{Fig_AIFsOQO.pstricks}
%	\input{image_Young_sssSing.pstricks}
\end{equation}
where the first line has $s_1$ boxes and the second one has $s_2$ boxes. Thanks to theorem \ref{ThoOpqrepreTens}, it can be realized with the tensor
\begin{equation}
	v_{a_1,\ldots a_{s_2}b_1\ldots s_1}(e_0)
\end{equation}
which is separately symmetric in the indices $a$ and $b$, in the same time as being antisymmetric in the couples $a_i$, $b_i$ when $i\leq s_2$, for example,
\begin{equation}
	v_{a_1\ldots a_{s_2},b_1\ldots b_{s_1}} = -v_{b_1 a_1\ldots a_{s_2},b_2\ldots b_{s_1}}.
\end{equation}
In particular, if we symmetrise $v$ on $s_1+1$ indices, we always found zero. Moreover, all the traces vanishes. If $\eta$ is the metric of $O(D-1)$, we have for example
\begin{equation}
	\eta^{b_1b_2}v_{a_1\ldots a_{s_2},b_1,\ldots b_{s_2}}=0.
\end{equation}

The vectors of the first level are the ones of the form	$L^+_r\ket{e_0,s}$. As far as notations are concerned, we have
\begin{equation}
	L^+_rv_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}=(L^+_rv)_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}.
\end{equation}
Remark that the operators $\{ L_r^+ \}_{r=1,\ldots,D-1}$ carry a representation of $o(D-1)$, namely the vector representation. Thus, the states of the first level form the representation given by the tensor product of $(s_1,s_2,0,\ldots)$ and the vector representation. In order to see the irreducible components of that representation, we have to know what are the symmetry properties that we can give to the indices
\begin{equation}
	r,a_1,\ldots,a_{s_2},b_1,\ldots,b_{s_1}.
\end{equation}
There are three possibilities: we can contract the $r$ with one of the $a_i$ (by symmetry, all of these contractions are equivalent), or with one of the $b_i$, or add one box in the Young diagram. The latter possibility splits into three cases: the diagram $(s_1,s_2,0,\ldots)$ can be transformed in $(s_1+1,s_2,0,\ldots)$, $(s_1,s_2+1,0,\ldots)$ or $(s_1,s_2,1,0,\ldots)$. So we have $5$ irreducible component in the $o(D-1)$ representation carried by the level one.

The question that naturally arises is to know if one of these have a singular vacuum. In other words, if $\Pi_{\beta}$ are the projections to the irreducible components, do we have
\begin{equation}
	L^-_{t}\Pi_{\alpha}\big( L^+_rv_{\bar a,\bar b}(e_0) \big)=0
\end{equation}
for a certain $e_0$ ?

Notice that the contraction with the last $b_i$'s is not the same as the one with the firsts ones because of the symmetry properties with respect to the $a_i$'s. The first representation with cell cut is given by
\begin{equation}
	v^1_{\bar a,b_1\ldots b_{s_1-1}}	=\eta^{rt}L^+_r\big\{ v_{\bar a,b_1 \ldots b_{s_1-1}t}(e_0)
							+\frac{ s_2 }{ s_1-s_2+1 } v_{ca_1\ldots a_{s_2-1},b_1\ldots b_{s_1-1}a_{s_2}(e_0)}\big\},
\end{equation}
while the second representation with cell cut is easier:
\begin{equation}
	v^2_{a_1\ldots a_{s_{2}-1},\bar b}=\eta^{rt}L^+_rv_{ta_1\ldots a_{s_2-1},\bar b}(e_0).
\end{equation}
Now, the sport is to compute $L^-_qv^1_{\bar a,b_1\ldots b_{s_1-1}}$ and $L^-_q v^2_{a_1\ldots a_{s_{2}-1},\bar b}$.

\begin{probleme}
Il y a du calcul non terminé, ici.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{The quotient for the scalar singleton}
%---------------------------------------------------------------------------------------------------------------------------

The value of the energy which saturates the unitary condition is $1$ when $s=\frac{1}{ 2 }$ and $\frac{ 1 }{2}$ when $s=0$. That is the reason why we consider the two special representations
\begin{equation}
	\begin{aligned}[]
		\rDi&=\mD(1,\frac{ 1 }{2})		&& \rRac&=\mD(\frac{ 1 }{2},0).
	\end{aligned}
\end{equation}
We are now interested in the scalar case, the $Rac$.

We know that, when $\epsilon_0$ is the value of the energy which saturates the unitary condition $e_0\geq \frac{ d-3 }{ 2 }$ (in the scalar case, then
\begin{enumerate}
\item the vectors $L^+_sL^+_s\ket{\epsilon_0}$ are singular vectors,
\item the vectors $L^+_{r_1}\cdots L^+_{r_n}L^+_sL^+_s\ket{\epsilon_0}$ is orthogonal to all other states, it is a null vector.
\end{enumerate}
On the other hand, we know from proposition \ref{PropSinModRedSSIADesNuls} that a module is reducible if and only if it has a vector $\ket{v}\neq\ket{e_0,s}$ such that $L^-_r\ket{v}=0$ for every $r$. Thus one constructs irreducible representations by taking the quotient of the Verma module by the singular module. 

What is the dimension of the scalar singleton ? We have to count how many different vectors we have in the Verma module $\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}$, and which \emph{are not} build over $L^+_sL^+_s\ket{e_0}$. In the case of $\SO(2,3)$, we have the generators $L^+_1$, $L^+_2$ and $L^+_3$ (which are commuting), so the only vectors that are left after removing the singular modules are the seven following ones: $L^+_1\ket{e_0}$, $L^+_2\ket{e_0}$,$L^+_3\ket{e_0}$, $L^+_1L^+_2\ket{e_0}$, $L^+_1L^+_3\ket{e_0}$,$L^+_2L^+_3\ket{e_0}$, and $L^+_1L^+_2L^+_3\ket{e_0}$. The scalar singleton representation is thus $7$ dimensional.

